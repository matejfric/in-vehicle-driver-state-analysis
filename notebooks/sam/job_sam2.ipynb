{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sam2.build_sam import build_sam2, build_sam2_video_predictor\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from sam2util import convert_images_to_mp4, sam2_output_export\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Use bfloat16 for the entire notebook\n",
    "torch.autocast(device_type='cuda', dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "if torch.cuda.get_device_properties(0).major >= 8:\n",
    "    # Turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "image_dir = Path.home() / 'source/driver-dataset/images/2021_09_06_poli_enyaq/normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_DIR = Path(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SAM2_MODEL = 'sam2_hiera_large'\n",
    "CONFIG = 'sam2_hiera_l.yaml'\n",
    "CHECKPOINT = Path.home() / f'source/driver-segmentation/segmentation-model/notebooks/sam/segment-anything-2/checkpoints/{SAM2_MODEL}.pt'\n",
    "\n",
    "assert CHECKPOINT.exists(), 'Checkpoint not found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam2_base = build_sam2(CONFIG, CHECKPOINT, device=DEVICE, apply_postprocessing=False)\n",
    "predictor = SAM2ImagePredictor(sam2_base)\n",
    "video_predictor = build_sam2_video_predictor(\n",
    "    CONFIG, CHECKPOINT, device=DEVICE, apply_postprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_names = list(IMAGES_DIR.glob('*.jpg'))\n",
    "# frame_names = [p for p in frame_names if int(p.stem) < IMAGES_LIMIT]\n",
    "frame_names.sort(key=lambda p: int(p.stem))\n",
    "\n",
    "# Visualize the first video frame\n",
    "frame_idx = 0\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(f'frame {frame_idx}')\n",
    "plt.imshow(Image.open(frame_names[frame_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_refinement = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_yolo = YOLO('/home/lanter/source/driver-segmentation/sam2/yolov8x.pt')\n",
    "person_cls_id = 0\n",
    "frame = frame_names[0]\n",
    "results = model_yolo(frame)[0]\n",
    "detections = sv.Detections.from_ultralytics(results)\n",
    "person_detections = detections[detections.class_id == 0]\n",
    "\n",
    "# Prepare bbox prompt for SAM\n",
    "person_boxes = results.boxes[results.boxes.cls == person_cls_id]\n",
    "sorted_indices = torch.argsort(person_boxes.conf, descending=True)\n",
    "bbox_prompt: np.ndarray = person_boxes[sorted_indices].xyxy.cpu().numpy()[0]\n",
    "\n",
    "print(bbox_prompt)\n",
    "\n",
    "image_rgb = Image.open(frame)\n",
    "predictor.set_image(image_rgb)\n",
    "\n",
    "masks, scores, logits = predictor.predict(\n",
    "    box=bbox_prompt,\n",
    "    multimask_output=False,\n",
    "    **mask_refinement,\n",
    ")\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "detections = sv.Detections(\n",
    "    xyxy=sv.mask_to_xyxy(masks=masks),\n",
    "    mask=masks.astype(bool),\n",
    ")\n",
    "\n",
    "source_image = box_annotator.annotate(scene=image_rgb.copy(), detections=detections)\n",
    "segmented_image = mask_annotator.annotate(scene=image_rgb.copy(), detections=detections)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[source_image, segmented_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image'],\n",
    ")\n",
    "\n",
    "print(masks.shape)\n",
    "mask_prompt = masks.squeeze()\n",
    "print(mask_prompt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del predictor\n",
    "del model_yolo\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This attempts to allocate ~75 GB of (video!) memory for 3:35 video (6450 frames)\n",
    "# Kernel crashed for 1000 frames, worked fine for 800\n",
    "\n",
    "inference_state = video_predictor.init_state(video_path=str(IMAGES_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mask prompt to the first frame\n",
    "_, out_obj_ids, out_mask_logits = video_predictor.add_new_mask(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=0,\n",
    "    obj_id=1,\n",
    "    mask=mask_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in video_predictor.propagate_in_video(\n",
    "    inference_state\n",
    "):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam2_output_export(video_segments, frame_names, IMAGES_DIR / 'sam2_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = IMAGES_DIR / 'sam2_output/visualization'\n",
    "output_video_path = IMAGES_DIR / 'sam2_output' / 'output.mp4'\n",
    "convert_images_to_mp4(image_folder, output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
