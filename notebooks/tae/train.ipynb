{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver State Anomaly Detection With Temporal Autoencoders\n",
    "\n",
    "[https://dagshub.com/matejfric/driver-state](https://dagshub.com/matejfric/driver-state)\n",
    "\n",
    "TODO: setup a pre-commit hook for https://github.com/mwouts/jupytext and Ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Literal, cast\n",
    "\n",
    "repo_root = str(Path.cwd().parent.parent)\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)\n",
    "\n",
    "import dagshub\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "import torchview\n",
    "from mlflow.models.signature import infer_signature\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "# Pytorch Lightning EarlyStopping callback does not recover the best weights as in Keras!\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# https://github.com/Lightning-AI/pytorch-lightning/discussions/10399,\n",
    "# https://pytorch-lightning.readthedocs.io/en/1.5.10/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from model import dmd\n",
    "from model.ae import (  # noqa F401\n",
    "    LSTMDecoder,\n",
    "    LSTMEncoder,\n",
    "    TemporalAutoencoderModel,\n",
    "    evaluate_model_parallel,\n",
    "    summarize_model,\n",
    ")\n",
    "from model.ae.iscv2023 import (  # noqa F401\n",
    "    EfficientNetEncoder,\n",
    "    ISVC23DecoderV1,\n",
    "    ISVC23DecoderV2,\n",
    "    ISVC23DecoderV3,\n",
    "    ISVC23DecoderV4,\n",
    "    ISVC23EncoderV1,\n",
    "    ISVC23EncoderV2,\n",
    "    ISVC23EncoderV3,\n",
    "    ISVC23EncoderV4,\n",
    ")\n",
    "from model.ae.temporal_3d import (\n",
    "    Conv3dDecoder,\n",
    "    Conv3dEncoder,\n",
    ")\n",
    "from model.common import Anomalies, BatchSizeDict\n",
    "from model.dataset import TemporalAutoencoderDataset, TemporalAutoencoderDatasetDMD\n",
    "from model.eval import compute_best_roc_auc\n",
    "from model.fonts import set_cmu_serif_font\n",
    "from model.git import get_commit_id, get_current_branch\n",
    "from model.logging import (\n",
    "    get_early_stopping_epoch,\n",
    "    get_experiment_id,\n",
    "    get_submodule_param_count,\n",
    "    log_dict_to_mlflow,\n",
    ")\n",
    "from model.plot import (\n",
    "    plot_error_and_anomalies,\n",
    "    plot_learning_curves,\n",
    "    plot_pr_chart,\n",
    "    plot_roc_chart,\n",
    "    plot_temporal_autoencoder_reconstruction,\n",
    "    show_examples,  # noqa F401 TODO\n",
    "    show_random,  # noqa F401 TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "set_cmu_serif_font()\n",
    "\n",
    "# Experiment logging\n",
    "REPO_NAME = 'driver-tae'\n",
    "USER_NAME = 'matejfric'\n",
    "dagshub.init(REPO_NAME, USER_NAME, mlflow=True)  # type: ignore\n",
    "\n",
    "# Reproducibility\n",
    "# https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility\n",
    "SEED = 42\n",
    "L.seed_everything(SEED, workers=True)\n",
    "\n",
    "print(\n",
    "    f'torch: {torch.__version__}, cuda: {torch.cuda.is_available()}, lightning: {L.__version__}'  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "driver = None\n",
    "source_type = None\n",
    "image_size = None\n",
    "latent_dim = None\n",
    "dataset = None\n",
    "batch_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "# ----------------------------------------\n",
    "MAX_EPOCHS = 100\n",
    "MIN_EPOCHS = 15\n",
    "MONITOR = 'valid_loss'\n",
    "PATIENCE = 10\n",
    "# Run memory map script to use a different image size (`run_memory_map_conversion.py`)\n",
    "IMAGE_SIZE: Literal[64, 128, 224, 256] = image_size or 64\n",
    "BATCH_SIZE = batch_size or 128\n",
    "SEQUENCE_LENGTH = 2\n",
    "TIME_STEP = 1\n",
    "LEARNING_RATE = 0.0005  # 1e-4\n",
    "LOSS_FUNCTION = 'mse'  # 'mae'\n",
    "TRAIN_SET_RATIO = 0.9\n",
    "USE_MASK = True\n",
    "MODEL_NAME: Literal['tae', 'tae3d'] = 'tae'\n",
    "LATENT_DIM = latent_dim or 2 * IMAGE_SIZE\n",
    "SOURCE_TYPE: Literal[\n",
    "    'depth',\n",
    "    'video_depth_anything',\n",
    "    'depth_realsense',\n",
    "    'masks',\n",
    "    'images',  # alias for 'rgb'\n",
    "    'rgb',\n",
    "    'rgbd',\n",
    "    'rgbdm',\n",
    "] = source_type or 'depth'\n",
    "if SOURCE_TYPE == 'images' or SOURCE_TYPE == 'rgb':\n",
    "    CHANNELS = 3\n",
    "elif SOURCE_TYPE == 'rgbd' or SOURCE_TYPE == 'rgb_source_depth':\n",
    "    CHANNELS = 4\n",
    "elif SOURCE_TYPE == 'rgbdm':\n",
    "    CHANNELS = 5\n",
    "else:\n",
    "    CHANNELS = 1\n",
    "\n",
    "# LOGGING\n",
    "# ----------------------------------------\n",
    "DRIVER_MAP = {\n",
    "    'geordi': '2021_08_31_geordi_enyaq',\n",
    "    'poli': '2021_09_06_poli_enyaq',\n",
    "    'michal': '2021_11_05_michal_enyaq',\n",
    "    'dans': '2021_11_18_dans_enyaq',\n",
    "    'jakub': '2021_11_18_jakubh_enyaq',\n",
    "    'radovan': '2024_07_02_radovan_enyaq',\n",
    "}\n",
    "NOW = datetime.datetime.now().strftime('%Y-%m-%d-%H%M%S')\n",
    "DRIVER = driver or 'geordi'\n",
    "EXPERIMENT_NAME = DRIVER if isinstance(DRIVER, str) else f'driver_{DRIVER}'\n",
    "LOG_DIR = Path('logs')\n",
    "RUN_NAME = (\n",
    "    f'{MODEL_NAME}-{DRIVER}-{SOURCE_TYPE}-{IMAGE_SIZE}x{IMAGE_SIZE}-latent{LATENT_DIM}'\n",
    ")\n",
    "VERSION = 0\n",
    "EXPERIMENT_DIR = LOG_DIR / f'{NOW}-{RUN_NAME}' / f'version_{VERSION}'\n",
    "\n",
    "\n",
    "MLFLOW_ARTIFACT_DIR = 'outputs'\n",
    "METRICS_CSV_NAME = 'metrics.csv'\n",
    "LEARNING_CURVES_PDF_NAME = 'learning_curves.pdf'\n",
    "PREDICTIONS_NAME = 'predictions.pdf'\n",
    "PREDICTIONS_JSON_NAME = 'predictions.json'\n",
    "TRAIN_TRANSFORMS_JSON_NAME = 'train_transforms.json'\n",
    "NOTEBOOK_NAME = 'train.ipynb'\n",
    "ARCHITECTURE_VISUALIZATION_NAME = 'architecture'\n",
    "MODEL_SUMMARY_NAME = 'model_summary.txt'\n",
    "ROC_CHART_NAME = 'roc_chart.pdf'\n",
    "ERROR_CHART_NAME = 'error_chart.pdf'\n",
    "PR_CHART_NAME = 'pr_chart.pdf'\n",
    "TEST_SESSION = None\n",
    "# MODEL_ONNX_NAME = 'model.onnx'\n",
    "\n",
    "# DATASET\n",
    "# ----------------------------------------\n",
    "DATASET: Literal['mrl', 'dmd'] = dataset or 'mrl'  #'mrl'\n",
    "DATASET_NAME = '2024-10-28-driver-all-frames' if DATASET == 'mrl' else 'dmd'\n",
    "DATASET_DIR = Path().home() / f'source/driver-dataset/{DATASET_NAME}'\n",
    "assert DATASET_DIR.exists(), f'Dataset directory does not exist: {DATASET_DIR}'\n",
    "\n",
    "[DRIVER_MAP[driver] for driver in ['geordi', 'poli', 'michal', 'dans', 'jakub']]\n",
    "\n",
    "if DATASET == 'mrl':\n",
    "    if DRIVER == 'all':\n",
    "        driver_dirs = [\n",
    "            DRIVER_MAP[driver]\n",
    "            for driver in ['geordi', 'poli', 'michal', 'dans', 'jakub']\n",
    "        ]\n",
    "    else:\n",
    "        driver_dirs = [DRIVER_MAP[DRIVER]]\n",
    "\n",
    "    TEST_SESSION = 'anomal'  # ~, 181149, 182201\n",
    "    memory_map_filename = (\n",
    "        f'{SOURCE_TYPE}_{IMAGE_SIZE}{\"\" if USE_MASK else \"_no_mask\"}.dat'\n",
    "    )\n",
    "\n",
    "    NORMAL_MEMORY_MAPS = []\n",
    "    ANOMAL_MEMORY_MAPS = []\n",
    "    ANOMALIES_FILES = []\n",
    "    for driver_dir in driver_dirs:\n",
    "        root_dir = DATASET_DIR / driver_dir\n",
    "        normal_mem_map = root_dir / 'normal' / 'memory_maps' / memory_map_filename\n",
    "        anomal_mem_map = root_dir / TEST_SESSION / 'memory_maps' / memory_map_filename\n",
    "        anomalies_file = root_dir / TEST_SESSION / 'labels.txt'\n",
    "\n",
    "        assert normal_mem_map.exists(), (\n",
    "            f'Normal memory map does not exist: {normal_mem_map}'\n",
    "        )\n",
    "        assert anomal_mem_map.exists(), (\n",
    "            f'Anomal memory map does not exist: {anomal_mem_map}'\n",
    "        )\n",
    "        assert anomalies_file.exists(), (\n",
    "            f'Anomalies file does not exist: {anomalies_file}'\n",
    "        )\n",
    "        NORMAL_MEMORY_MAPS.append(normal_mem_map)\n",
    "        ANOMAL_MEMORY_MAPS.append(anomal_mem_map)\n",
    "        ANOMALIES_FILES.append(anomalies_file)\n",
    "\n",
    "elif DATASET == 'dmd':\n",
    "    TRAIN_SESSIONS = sorted(dmd.DRIVER_SESSION_MAPPING[DRIVER])\n",
    "    # TEST_SESSIONS = copy.copy(TRAIN_SESSIONS)\n",
    "\n",
    "    # Use session 's1' for testing.\n",
    "    TEST_SESSIONS = [x for x in TRAIN_SESSIONS if 's1' in x]\n",
    "    TRAIN_SESSIONS = [x for x in TRAIN_SESSIONS if 's1' not in x]\n",
    "\n",
    "    # assert SESSION not in TEST_SESSIONS, 'Training session cannot be in the test set!'\n",
    "\n",
    "    TRAIN_DATASETS = sorted(\n",
    "        [DATASET_DIR / session / 'normal' for session in TRAIN_SESSIONS]\n",
    "    )\n",
    "    assert all(dataset.exists() for dataset in TRAIN_DATASETS), (\n",
    "        'Training datasets do not exist!'\n",
    "    )\n",
    "\n",
    "    TEST_DATASETS = sorted(\n",
    "        [DATASET_DIR / session / 'memory_maps' for session in TEST_SESSIONS]\n",
    "    )\n",
    "    assert all(dataset.exists() for dataset in TEST_DATASETS), (\n",
    "        'Test datasets do not exist!'\n",
    "    )\n",
    "\n",
    "    ANOMALIES_FILES = [\n",
    "        dataset.parent / f'{dataset.parent.name}.json'\n",
    "        for dataset in TEST_DATASETS\n",
    "        if dataset.is_dir()\n",
    "    ]\n",
    "    assert all(anom_file.exists() for anom_file in ANOMALIES_FILES), (\n",
    "        f'Anomalies file does not exist: {ANOMALIES_FILES}'\n",
    "    )\n",
    "\n",
    "if MODEL_NAME == 'tae3d':\n",
    "    assert SEQUENCE_LENGTH >= 16, (\n",
    "        'Number of time steps must be at least 16 for the 3D convolution model'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test case for forward pass, also used as model signature in Mlflow.\n",
    "\n",
    "if MODEL_NAME == 'tae':\n",
    "    # encoder = LSTMEncoder(n_time_steps=SEQUENCE_LENGTH, bidirectional=True)\n",
    "    # decoder = LSTMDecoder(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH,\n",
    "    #     n_image_channels=1,\n",
    "    #     image_size=IMAGE_SIZE,\n",
    "    #     bidirectional=True,\n",
    "    # )\n",
    "\n",
    "    # encoder = EfficientNetEncoder(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH,\n",
    "    #     bidirectional=True,\n",
    "    #     image_size=IMAGE_SIZE,\n",
    "    #     latent_dim=LATENT_DIM,\n",
    "    # )\n",
    "\n",
    "    encoder = ISVC23EncoderV1(\n",
    "        n_time_steps=SEQUENCE_LENGTH,\n",
    "        bidirectional=True,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        latent_dim=LATENT_DIM,\n",
    "    )\n",
    "\n",
    "    decoder = ISVC23DecoderV1(\n",
    "        n_time_steps=SEQUENCE_LENGTH,\n",
    "        bidirectional=True,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        n_image_channels=CHANNELS,\n",
    "    )\n",
    "\n",
    "    # encoder = ISVC23EncoderV2(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH, image_size=IMAGE_SIZE, latent_dim=LATENT_DIM\n",
    "    # )\n",
    "    # decoder = ISVC23DecoderV2(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH, image_size=IMAGE_SIZE, latent_dim=LATENT_DIM\n",
    "    # )\n",
    "\n",
    "    # encoder = ISVC23EncoderV3(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH, image_size=IMAGE_SIZE, latent_dim=LATENT_DIM\n",
    "    # )\n",
    "    # decoder = ISVC23DecoderV3(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH, image_size=IMAGE_SIZE, latent_dim=LATENT_DIM\n",
    "    # )\n",
    "\n",
    "    # encoder = ISVC23EncoderV4(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH, image_size=IMAGE_SIZE, latent_dim=LATENT_DIM\n",
    "    # )\n",
    "    # decoder = ISVC23DecoderV4(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH, image_size=IMAGE_SIZE, latent_dim=LATENT_DIM\n",
    "    # )\n",
    "\n",
    "    # Test input tensor of size (batch_size, time_steps, channels, height, width)\n",
    "    INPUT_SAMPLE = torch.randn(\n",
    "        BATCH_SIZE, SEQUENCE_LENGTH, CHANNELS, IMAGE_SIZE, IMAGE_SIZE\n",
    "    )\n",
    "\n",
    "    # Forward pass through the encoder and decoder\n",
    "    encoded = encoder(INPUT_SAMPLE)\n",
    "    decoded = decoder(encoded)\n",
    "\n",
    "    print(f'Encoder: {encoder.__class__.__name__}')\n",
    "    print(f'Decoder: {decoder.__class__.__name__}')\n",
    "\n",
    "    # Check the shapes\n",
    "    print(f'Input shape: {INPUT_SAMPLE.shape}')\n",
    "    print(f'Latent shape: {encoded.shape}')\n",
    "    print(f'Decoded shape: {decoded.shape}')\n",
    "\n",
    "    assert INPUT_SAMPLE.shape == decoded.shape, 'Input and output shapes do not match!'\n",
    "\n",
    "    print(summarize_model([encoder, decoder]))\n",
    "\n",
    "    # torchinfo.summary(\n",
    "    #     encoder,\n",
    "    #     input_size=(BATCH_SIZE, SEQUENCE_LENGTH, 1, IMAGE_SIZE, IMAGE_SIZE),\n",
    "    #     depth=4,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test case for forward pass, also used as model signature in Mlflow.\n",
    "\n",
    "if MODEL_NAME == 'tae3d':\n",
    "    encoder = Conv3dEncoder()\n",
    "    decoder = Conv3dDecoder()\n",
    "\n",
    "    # Test input tensor of size (batch_size, channels, time_steps/depth, height, width)\n",
    "    INPUT_SAMPLE = torch.randn(\n",
    "        BATCH_SIZE, CHANNELS, SEQUENCE_LENGTH, IMAGE_SIZE, IMAGE_SIZE\n",
    "    )\n",
    "\n",
    "    # Forward pass through the encoder and decoder\n",
    "    encoded = encoder(INPUT_SAMPLE)\n",
    "    decoded = decoder(encoded)\n",
    "\n",
    "    # Check the shapes\n",
    "    print(f'Input shape: {INPUT_SAMPLE.shape}')\n",
    "    print(f'Latent shape: {encoded.shape}')\n",
    "    print(f'Decoded shape: {decoded.shape}')\n",
    "\n",
    "    assert INPUT_SAMPLE.shape == decoded.shape, 'Input and output shapes do not match!'\n",
    "\n",
    "    print(summarize_model([encoder, decoder]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_DIM_INDEX = 0 if MODEL_NAME == 'tae' else 1\n",
    "DTYPE = np.float32 if SOURCE_TYPE == 'depth_realsense' else np.uint8\n",
    "\n",
    "batch_size_dict = BatchSizeDict(\n",
    "    {'train': BATCH_SIZE, 'valid': BATCH_SIZE, 'test': BATCH_SIZE}\n",
    ")\n",
    "\n",
    "if DATASET == 'dmd':\n",
    "    train_val_dataset = TemporalAutoencoderDatasetDMD(\n",
    "        dataset_directories=TRAIN_DATASETS,\n",
    "        memory_map_image_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS),\n",
    "        window_size=SEQUENCE_LENGTH,\n",
    "        time_step=TIME_STEP,\n",
    "        time_dim_index=TIME_DIM_INDEX,\n",
    "        source_type=SOURCE_TYPE,\n",
    "    )\n",
    "    test_dataset = TemporalAutoencoderDatasetDMD(\n",
    "        dataset_directories=TEST_DATASETS,\n",
    "        memory_map_image_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS),\n",
    "        window_size=SEQUENCE_LENGTH,\n",
    "        time_step=TIME_STEP,\n",
    "        time_dim_index=TIME_DIM_INDEX,\n",
    "        source_type=SOURCE_TYPE,\n",
    "    )\n",
    "elif DATASET == 'mrl':\n",
    "    train_val_dataset = TemporalAutoencoderDataset(\n",
    "        memory_map_file=NORMAL_MEMORY_MAPS,\n",
    "        memory_map_image_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS),\n",
    "        window_size=SEQUENCE_LENGTH,\n",
    "        time_step=TIME_STEP,\n",
    "        time_dim_index=TIME_DIM_INDEX,\n",
    "        dtype=DTYPE,\n",
    "    )\n",
    "    test_dataset = TemporalAutoencoderDataset(\n",
    "        memory_map_file=ANOMAL_MEMORY_MAPS,\n",
    "        memory_map_image_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS),\n",
    "        window_size=SEQUENCE_LENGTH,\n",
    "        time_step=TIME_STEP,\n",
    "        time_dim_index=TIME_DIM_INDEX,\n",
    "        dtype=DTYPE,\n",
    "    )\n",
    "\n",
    "# Train validation split\n",
    "train_size = int(TRAIN_SET_RATIO * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size_dict['train'],\n",
    "    shuffle=False,  # YOU DON'T WANT TO SHUFFLE TEMPORAL DATA!\n",
    "    num_workers=int(os.cpu_count()),  # type: ignore\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size_dict['valid'],\n",
    "    shuffle=False,\n",
    "    num_workers=int(os.cpu_count()),  # type: ignore\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # batch_size_dict['test'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # int(os.cpu_count()),  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example\n",
    "plt.imshow(test_dataloader.dataset[0]['image'][0].permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalAutoencoderModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    batch_size_dict=batch_size_dict,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    loss_function=LOSS_FUNCTION,\n",
    "    time_dim_index=1 if MODEL_NAME == 'tae' else 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(LOG_DIR, name=f'{NOW}-{RUN_NAME}', version=VERSION)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=MONITOR,\n",
    "    mode='min',\n",
    "    patience=PATIENCE,\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=EXPERIMENT_DIR,\n",
    "    filename='{epoch}-{valid_loss:3f}',\n",
    "    monitor=MONITOR,\n",
    "    save_top_k=1,  # save only the best model\n",
    "    mode='min',\n",
    ")\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',\n",
    "    logger=csv_logger,\n",
    "    callbacks=[model_checkpoint, early_stopping, progress_bar],\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    min_epochs=MIN_EPOCHS,\n",
    "    log_every_n_steps=1,  # log every batch\n",
    "    # https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility\n",
    "    deterministic=True,\n",
    ")\n",
    "torch.use_deterministic_algorithms(\n",
    "    True, warn_only=True\n",
    ")  # torch 2.5 does not implement deterministic `max_pool3d_with_indices_backward_cuda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time = datetime.datetime.now()\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader,\n",
    ")\n",
    "\n",
    "training_time = datetime.datetime.now() - training_time\n",
    "training_time_minutes = training_time.total_seconds() / 60\n",
    "print(f'Training time: {training_time_minutes:.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Load from MLflow\n",
    "    model_name = 'pytorch-2025-02-16-201321-tae-radovan-isvc23v1-L515-test'\n",
    "    model_version = 1\n",
    "    model_uri = f'models:/{model_name}/{model_version}'\n",
    "    model_ = mlflow.pytorch.load_model(model_uri)\n",
    "else:\n",
    "    model_checkpoint_path = list(EXPERIMENT_DIR.glob('*.ckpt'))[0]\n",
    "    print(f'Loading model from: {model_checkpoint_path}')\n",
    "    model_ = TemporalAutoencoderModel.load_from_checkpoint(\n",
    "        model_checkpoint_path, encoder=encoder, decoder=decoder\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ = L.Trainer(logger=False)  # no need to log anything for validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metrics = trainer_.validate(model_, dataloaders=valid_dataloader, verbose=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "if DATASET == 'mrl':\n",
    "    anomalies = Anomalies.from_file(ANOMALIES_FILES)\n",
    "elif DATASET == 'dmd':\n",
    "    video_lengths = [vid.length for vid in test_dataset.videos]  # type: ignore\n",
    "    # Anomaly files are sorted so that they correspond to the video lengths.\n",
    "    anomalies = Anomalies.from_json(ANOMALIES_FILES, video_lengths)\n",
    "\n",
    "y_true = anomalies.to_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = evaluate_model_parallel(model_, test_dataloader)\n",
    "test_metrics = {\n",
    "    'mse': sum(errors['mse']) / len(errors['mse']),\n",
    "    'mae': sum(errors['mae']) / len(errors['mae']),\n",
    "    'fro': sum(errors['fro']) / len(errors['fro']),\n",
    "}\n",
    "for metric_name, value in test_metrics.items():\n",
    "    print(f'{metric_name}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = compute_best_roc_auc(y_true, errors)\n",
    "best_metric = str(res_dict['best_metric'])\n",
    "y_proba = cast(list[float], res_dict['y_proba'])\n",
    "y_true = y_true[: len(y_proba)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc, optimal_threshold = plot_roc_chart(\n",
    "    y_true=y_true,\n",
    "    y_pred_proba=y_proba,\n",
    "    save_path=EXPERIMENT_DIR / ROC_CHART_NAME,\n",
    "    cbar_text=f'Thresholds ({best_metric.upper()})',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "predictions = {\n",
    "    'y_true': y_true,\n",
    "    'y_proba': y_proba,\n",
    "    'errors': errors,\n",
    "    'roc_auc': roc_auc,\n",
    "    'best_metric': best_metric,\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "}\n",
    "with open(EXPERIMENT_DIR / PREDICTIONS_JSON_NAME, 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_auc, pr_threshold = plot_pr_chart(\n",
    "    y_true=y_true,\n",
    "    y_pred_proba=y_proba,\n",
    "    save_path=EXPERIMENT_DIR / PR_CHART_NAME,\n",
    "    figsize=(8, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_and_anomalies(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_proba,\n",
    "    threshold=optimal_threshold,\n",
    "    save_path=EXPERIMENT_DIR / ERROR_CHART_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_temporal_autoencoder_reconstruction(\n",
    "    model_,\n",
    "    test_dataloader,\n",
    "    save_path=EXPERIMENT_DIR / PREDICTIONS_NAME,\n",
    "    time_dim_index=0 if MODEL_NAME == 'tae' else 1,\n",
    "    indices=(\n",
    "        [0]  # first video frame\n",
    "        + [anomalies[0].middle()]\n",
    "        + [anomalies[1].middle()]\n",
    "        + [anomalies[2].middle()]\n",
    "        + [anomalies[-2].middle()]\n",
    "        + [anomalies[-1].middle()]\n",
    "    ),\n",
    "    show_heatmap=True,\n",
    "    show_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(\n",
    "    EXPERIMENT_DIR / METRICS_CSV_NAME,\n",
    "    save_path=EXPERIMENT_DIR / LEARNING_CURVES_PDF_NAME,\n",
    "    metrics={\n",
    "        #'mse': 'Mean Squared Error',\n",
    "        #'fro': 'Frobenius Norm',\n",
    "        'mae': 'MAE',\n",
    "    },\n",
    "    figsize=(16, 4.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_visualization = torchview.draw_graph(\n",
    "    model_,\n",
    "    input_size=INPUT_SAMPLE.shape,\n",
    "    graph_dir='TB',\n",
    "    depth=3,\n",
    "    roll=True,\n",
    "    expand_nested=True,\n",
    "    graph_name='Temporal Autoencoder',\n",
    "    save_graph=False,\n",
    "    filename=ARCHITECTURE_VISUALIZATION_NAME,\n",
    "    directory=str(EXPERIMENT_DIR),\n",
    ")\n",
    "architecture_visualization.visual_graph.render(format='svg')\n",
    "architecture_visualization.visual_graph.render(format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_.to_onnx(\n",
    "#     EXPERIMENT_DIR / MODEL_ONNX_NAME,\n",
    "#     INPUT_SAMPLE,\n",
    "#     export_params=False,\n",
    "#     dynamo=False,\n",
    "#     opset_version=11,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPERIMENT_DIR / MODEL_SUMMARY_NAME, 'w') as f:\n",
    "    f.write(summarize_model([encoder, decoder]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(\n",
    "    run_name=f'{RUN_NAME}', experiment_id=get_experiment_id(EXPERIMENT_NAME)\n",
    ") as run:\n",
    "    try:\n",
    "        mlflow.set_tag('branch', get_current_branch())\n",
    "        mlflow.set_tag('commit_id', get_commit_id())\n",
    "        mlflow.set_tag('dataset_name', DATASET_NAME)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    mlflow.log_metric('roc_auc', roc_auc)\n",
    "    mlflow.log_metric('optimal_threshold', optimal_threshold)\n",
    "    mlflow.log_metric('pr_auc', pr_auc)\n",
    "    mlflow.log_metric('pr_threshold', pr_threshold)\n",
    "    log_dict_to_mlflow(valid_metrics, 'metric')\n",
    "    log_dict_to_mlflow(test_metrics, 'metric')\n",
    "    log_dict_to_mlflow(get_submodule_param_count(model_), 'param')\n",
    "\n",
    "    mlflow.log_param('dataset', DATASET)\n",
    "    mlflow.log_param('annotations', ANOMALIES_FILES)\n",
    "    mlflow.log_param('encoder', str(encoder))\n",
    "    mlflow.log_param('decoder', str(decoder))\n",
    "    mlflow.log_param('encoder_name', encoder.__class__.__name__)\n",
    "    mlflow.log_param('decoder_name', decoder.__class__.__name__)\n",
    "    mlflow.log_param('batch_size', BATCH_SIZE)\n",
    "    mlflow.log_param('max_epochs', MAX_EPOCHS)\n",
    "    mlflow.log_param('min_epochs', MIN_EPOCHS)\n",
    "    mlflow.log_param('early_stopping', get_early_stopping_epoch(EXPERIMENT_DIR))\n",
    "    mlflow.log_param('monitor', MONITOR)\n",
    "    mlflow.log_param('patience', PATIENCE)\n",
    "    mlflow.log_param('image_size', IMAGE_SIZE)\n",
    "    mlflow.log_param('learning_rate', LEARNING_RATE)\n",
    "    mlflow.log_param('loss_function', LOSS_FUNCTION)\n",
    "    mlflow.log_param('seed', SEED)\n",
    "    mlflow.log_param('sequence_length', SEQUENCE_LENGTH)\n",
    "    mlflow.log_param('time_step', TIME_STEP)\n",
    "    mlflow.log_param('train_set_ratio', TRAIN_SET_RATIO)\n",
    "    mlflow.log_param('train_sequences', len(train_dataset))\n",
    "    mlflow.log_param('driver', DRIVER)\n",
    "    mlflow.log_param('best_metric', best_metric)\n",
    "    mlflow.log_param('use_mask', USE_MASK)\n",
    "    mlflow.log_param('source_type', SOURCE_TYPE)\n",
    "    mlflow.log_param('latent_dim', LATENT_DIM)\n",
    "    mlflow.log_param('training_time_minutes', training_time_minutes)\n",
    "    mlflow.log_param('model_name', MODEL_NAME)\n",
    "    mlflow.log_param('channels', CHANNELS)\n",
    "\n",
    "    if DATASET == 'mrl':\n",
    "        mlflow.log_param('test_session', TEST_SESSION)\n",
    "\n",
    "    elif DATASET == 'dmd':\n",
    "        mlflow.log_param('train_sessions', TRAIN_SESSIONS)\n",
    "        mlflow.log_param('test_sessions', TEST_SESSIONS)\n",
    "        mlflow.log_param('train_datasets', [str(dataset) for dataset in TRAIN_DATASETS])\n",
    "        mlflow.log_param('test_datasets', [str(dataset) for dataset in TEST_DATASETS])\n",
    "\n",
    "    # CSV metrics, learning curves, predictions, notebook\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / METRICS_CSV_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / LEARNING_CURVES_PDF_NAME), MLFLOW_ARTIFACT_DIR\n",
    "    )\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / PREDICTIONS_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / ROC_CHART_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / MODEL_SUMMARY_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / ERROR_CHART_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / PR_CHART_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / PREDICTIONS_JSON_NAME), MLFLOW_ARTIFACT_DIR\n",
    "    )\n",
    "    mlflow.log_artifact(str(Path().cwd() / NOTEBOOK_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "\n",
    "    # Network visualization with `torchview`\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / (ARCHITECTURE_VISUALIZATION_NAME + '.svg')),\n",
    "        MLFLOW_ARTIFACT_DIR,\n",
    "    )\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / (ARCHITECTURE_VISUALIZATION_NAME + '.png')),\n",
    "        MLFLOW_ARTIFACT_DIR,\n",
    "    )\n",
    "\n",
    "    # Models are versioned by default\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model=model_,\n",
    "        artifact_path='model',\n",
    "        registered_model_name=f'pytorch-{DRIVER}',\n",
    "        signature=infer_signature(\n",
    "            INPUT_SAMPLE.numpy(), INPUT_SAMPLE.numpy(), dict(training=False)\n",
    "        ),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
