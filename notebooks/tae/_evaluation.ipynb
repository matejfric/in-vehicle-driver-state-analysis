{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of TAE Experiments\n",
    "\n",
    "This notebook connects to MLflow, downloads all experiment runs and creates visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import dagshub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlflow.client import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_curve\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME = 'driver-tae'\n",
    "USER_NAME = 'matejfric'\n",
    "dagshub.init(REPO_NAME, USER_NAME, mlflow=True)  # type: ignore\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = client.search_experiments(filter_string=\"name!='Default'\")\n",
    "pprint([experiment.name for experiment in experiments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all runs from the experiments\n",
    "all_runs = []\n",
    "for experiment in experiments:\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        filter_string='',\n",
    "        run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    )\n",
    "    all_runs.extend(runs)\n",
    "\n",
    "# Create a DataFrame from the runs\n",
    "runs_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'run_id': r.info.run_id,\n",
    "            'experiment_id': r.info.experiment_id,\n",
    "            'experiment_name': client.get_experiment(r.info.experiment_id).name,\n",
    "            'status': r.info.status,\n",
    "            'start_time': pd.to_datetime(r.info.start_time, unit='ms'),\n",
    "            'end_time': pd.to_datetime(r.info.end_time, unit='ms')\n",
    "            if r.info.end_time\n",
    "            else None,\n",
    "            'artifact_uri': r.info.artifact_uri,\n",
    "            **r.data.params,  # Add all parameters\n",
    "            **{\n",
    "                f'metric.{k}': v for k, v in r.data.metrics.items()\n",
    "            },  # Add all metrics with \"metric.\" prefix\n",
    "        }\n",
    "        for r in all_runs\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(f'Total runs: {len(runs_df)}')\n",
    "runs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_columns = ['image_size', 'latent_dim', 'batch_size', 'early_stopping']\n",
    "runs_df[integer_columns] = runs_df[integer_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = runs_df.groupby(['driver', 'source_type', 'latent_dim', 'image_size'])[\n",
    "    'metric.roc_auc'\n",
    "].idxmax()\n",
    "best_runs_df = runs_df.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = best_runs_df[\n",
    "    (best_runs_df['image_size'] == 64)\n",
    "    & (best_runs_df['latent_dim'] == 128)\n",
    "    & (best_runs_df['dataset'] != 'dmd')\n",
    "]\n",
    "df[\n",
    "    [\n",
    "        'driver',\n",
    "        'source_type',\n",
    "        'metric.roc_auc',\n",
    "        'early_stopping',\n",
    "        'patience',\n",
    "        'min_epochs',\n",
    "        'best_metric',\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(local_path=None)\n",
    "local_root = Path.cwd() / 'outputs' / 'mlflow_artifacts'\n",
    "artifact_dir = 'outputs/'\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    run_id = row['run_id']\n",
    "    # Download artifacts and store the path\n",
    "    local_dir = local_root / str(run_id)\n",
    "    local_dir.mkdir(parents=True, exist_ok=True)\n",
    "    local_path = client.download_artifacts(\n",
    "        run_id, artifact_dir + 'predictions.json', str(local_dir)\n",
    "    )\n",
    "    # Save the local path to the dataframe\n",
    "    df.at[index, 'local_path'] = local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_type_map = {\n",
    "    'depth': 'Depth',\n",
    "    'images': 'RGB',\n",
    "    'masks': 'Mask',\n",
    "    'rgbd': 'RGBD',\n",
    "    'rgbdm': 'RGBDM',\n",
    "}\n",
    "source_type_color_map = {\n",
    "    'Depth': 'tab:blue',\n",
    "    'RGB': 'tab:orange',\n",
    "    'Mask': 'tab:green',\n",
    "    'RGBD': 'tab:red',\n",
    "    'RGBDM': 'tab:purple',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predictions from the local paths\n",
    "data = defaultdict(dict)\n",
    "for index, row in df.iterrows():\n",
    "    with open(row['local_path']) as f:\n",
    "        results = json.load(f)\n",
    "    data[row['driver']][source_type_map[row['source_type']]] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers = list(data.keys())\n",
    "source_types = list(data[list(data.keys())[0]].keys())\n",
    "pprint(source_types)\n",
    "pprint(drivers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc_chart(\n",
    "    data: dict[str, dict[str, dict[str, list[float | int] | float]]],\n",
    "    cmap: str = 'rainbow',\n",
    "    figsize: tuple[int, int] | None = None,\n",
    "    plot_thresholds: bool = False,\n",
    "    cbar_text: str = 'Threshold',\n",
    "    save_path: str | Path | None = None,\n",
    "    justification: int = 5,\n",
    ") -> None:\n",
    "    n_plots = len(data)\n",
    "\n",
    "    # Calculate default figsize if not provided\n",
    "    if figsize is None:\n",
    "        figsize = (7 * n_plots, 7)\n",
    "\n",
    "    # Create figure with gridspec to accommodate colorbar\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = plt.GridSpec(1, n_plots + 1, width_ratios=[1] * n_plots + [0.05])  # type: ignore\n",
    "    axes = [fig.add_subplot(gs[0, i]) for i in range(n_plots)]\n",
    "\n",
    "    # Use fixed colormap range from 0 to 1\n",
    "    norm = plt.Normalize(vmin=0, vmax=1)  # type: ignore\n",
    "\n",
    "    for idx, (ax, (driver_name, driver_results)) in enumerate(zip(axes, data.items())):\n",
    "        for source_type, results in driver_results.items():\n",
    "            y_true: list[int] = results['y_true']  # type: ignore\n",
    "            y_pred_proba: list[float] = results['y_proba']  # type: ignore\n",
    "            title = driver_name.capitalize()\n",
    "\n",
    "            # Calculate ROC curve\n",
    "            fpr, tpr, thresholds = roc_curve(y_true[: len(y_pred_proba)], y_pred_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "\n",
    "            # Plot ROC curve\n",
    "            ax.plot(\n",
    "                fpr,\n",
    "                tpr,\n",
    "                c=source_type_color_map[source_type],\n",
    "                label=f'{source_type.ljust(justification)} AUC={roc_auc:.3f}',\n",
    "                linewidth=2,\n",
    "            )\n",
    "\n",
    "            if plot_thresholds:\n",
    "                scatter = ax.scatter(fpr, tpr, c=thresholds, cmap=cmap, norm=norm)\n",
    "\n",
    "            # Random predictions curve\n",
    "            ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "\n",
    "        # Set title and limits\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlim([0, 1])  # type: ignore\n",
    "        ax.set_ylim([0, 1])  # type: ignore\n",
    "        ax.axis('square')\n",
    "\n",
    "        # Handle axis labels and ticks\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('True positive rate')\n",
    "        else:\n",
    "            # Remove y-axis labels for all but the first plot\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "        # Add x-label to all plots\n",
    "        ax.set_xlabel('False positive rate')\n",
    "        ax.legend(loc='lower right', prop={'family': 'monospace'})\n",
    "\n",
    "    # Add colorbar in the last column of gridspec\n",
    "    if plot_thresholds:\n",
    "        cbar_ax = fig.add_subplot(gs[0, -1])\n",
    "        cbar = fig.colorbar(scatter, cax=cbar_ax)\n",
    "        cbar.set_label(cbar_text)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_auc_chart(data, save_path='outputs/roc_auc.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_auc_chart(\n",
    "    data: dict[str, dict[str, dict[str, list[float | int] | float]]],\n",
    "    cmap: str = 'rainbow',\n",
    "    figsize: tuple[int, int] | None = None,\n",
    "    plot_thresholds: bool = False,\n",
    "    cbar_text: str = 'Threshold',\n",
    "    save_path: str | Path | None = None,\n",
    "    justification: int = 5,\n",
    ") -> None:\n",
    "    n_plots = len(data)\n",
    "\n",
    "    # Calculate default figsize if not provided\n",
    "    if figsize is None:\n",
    "        figsize = (7 * n_plots, 7)\n",
    "\n",
    "    # Create figure with gridspec to accommodate colorbar\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = plt.GridSpec(  # type: ignore\n",
    "        1,\n",
    "        n_plots + 1,\n",
    "        width_ratios=[1] * n_plots + ([0.05] if plot_thresholds else [0.0]),\n",
    "    )\n",
    "    axes = [fig.add_subplot(gs[0, i]) for i in range(n_plots)]\n",
    "\n",
    "    # Use fixed colormap range from 0 to 1\n",
    "    norm = plt.Normalize(vmin=0, vmax=1)  # type: ignore\n",
    "\n",
    "    for idx, (ax, (driver_name, driver_results)) in enumerate(zip(axes, data.items())):\n",
    "        for source_type, results in driver_results.items():\n",
    "            y_true: list[int] = results['y_true']  # type: ignore\n",
    "            y_pred_proba: list[float] = results['y_proba']  # type: ignore\n",
    "            title = driver_name.capitalize()\n",
    "\n",
    "            # Calculate ROC curve\n",
    "            precision, recall, thresholds = precision_recall_curve(\n",
    "                y_true[: len(y_pred_proba)], y_pred_proba\n",
    "            )\n",
    "            precision[precision > 1.0] = 1.0\n",
    "            recall[recall > 1.0] = 1.0\n",
    "            pr_auc = auc(recall, precision)\n",
    "\n",
    "            # Compute F1 scores for each threshold (skip the first element).\n",
    "            with warnings.catch_warnings():\n",
    "                # Suppress warnings for division by zero\n",
    "                warnings.simplefilter('ignore')\n",
    "                f1_scores = (\n",
    "                    2 * precision[1:] * recall[1:] / (precision[1:] + recall[1:])\n",
    "                )\n",
    "            # from sklearn.metrics import f1_score\n",
    "            # f1_scores = []\n",
    "            # for th in thresholds:\n",
    "            #     y_pred = [1 if p >= th else 0 for p in y_pred_proba]\n",
    "            #     f1 = f1_score(y_true, y_pred)\n",
    "            #     f1_scores.append(f1)\n",
    "            optimal_idx = np.array(f1_scores).argmax()\n",
    "            optimal_threshold = thresholds[optimal_idx]  # noqa\n",
    "\n",
    "            # Plot the precision-recall curve.\n",
    "            ax.step(\n",
    "                recall,\n",
    "                precision,\n",
    "                c=source_type_color_map[source_type],\n",
    "                label=f'{source_type.ljust(justification)} AUC={pr_auc:.3f}',\n",
    "                linewidth=2,\n",
    "            )\n",
    "\n",
    "            if plot_thresholds:\n",
    "                scatter = plt.scatter(\n",
    "                    recall[1:], precision[1:], c=thresholds, cmap=cmap, norm=norm\n",
    "                )\n",
    "\n",
    "        # Set title and limits\n",
    "        ax.set_title(title)\n",
    "\n",
    "        ax.set_xlim([0, 1.1])  # type: ignore\n",
    "        ax.set_ylim([0, 1.1])  # type: ignore\n",
    "\n",
    "        ax.axis('tight')\n",
    "\n",
    "        ax.set_xticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        ax.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Handle axis labels and ticks\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('Precision')\n",
    "        else:\n",
    "            # Remove y-axis labels for all but the first plot\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "        # Add x-label to all plots\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.legend(\n",
    "            prop={'family': 'monospace'},\n",
    "            loc='upper center',\n",
    "            bbox_to_anchor=(0.5, -0.15),\n",
    "        )\n",
    "\n",
    "    # Add colorbar in the last column of gridspec\n",
    "    if plot_thresholds:\n",
    "        cbar_ax = fig.add_subplot(gs[0, -1])\n",
    "        cbar = fig.colorbar(scatter, cax=cbar_ax)\n",
    "        cbar.set_label(cbar_text)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "    # plt.tight_layout(rect=[0, 0.05, 1, 1])  # Add padding at the bottom for the legends\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr_auc_chart(data, save_path='outputs/pr_auc.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
