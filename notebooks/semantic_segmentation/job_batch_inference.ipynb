{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Batch Inference With Segmentation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "from collections.abc import Generator\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from os import cpu_count\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "repo_root = str(Path.cwd().parent.parent)\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from PIL import Image\n",
    "from sam2util import convert_images_to_mp4\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model.common import crop_driver_image_contains\n",
    "from model.inference import (\n",
    "    filter_small_segments,\n",
    "    init_dagshub,\n",
    "    load_model_from_dagshub,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_name = 'pytorch-2025-02-28-173314-unetplusplus-efficientnet-b1'\n",
    "model_version = 'champion'\n",
    "input_dir = (\n",
    "    Path().home()\n",
    "    / 'source/driver-dataset/2024-10-28-driver-all-frames/2021_08_31_geordi_enyaq/normal/images'\n",
    ")\n",
    "batch_size = 16\n",
    "repo_name = 'driver-seg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = Path(input_dir)\n",
    "\n",
    "OUTPUT_DIR = INPUT_DIR.parent / 'masks'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "BATCH_SIZE = batch_size if isinstance(batch_size, int) else 16\n",
    "\n",
    "assert INPUT_DIR.exists(), f'Input directory `{INPUT_DIR}` does not exist.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_dagshub(repo_name=repo_name)\n",
    "model_ = load_model_from_dagshub(model_name, model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_image(\n",
    "    image_path: Path, resize: tuple[int, int], transforms: Any\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Helper function to load and process a single image.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    processed_image = crop_driver_image_contains(image, image_path).resize(\n",
    "        resize, resample=Image.NEAREST\n",
    "    )\n",
    "    return transforms(image=np.asarray(processed_image))['image']\n",
    "\n",
    "\n",
    "def data_loader(\n",
    "    image_paths: list[Path],\n",
    "    batch_size: int = 32,\n",
    "    resize: tuple[int, int] = (256, 256),\n",
    "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    ") -> Generator[tuple[torch.Tensor, list[Path]]]:\n",
    "    transforms = ToTensorV2()\n",
    "\n",
    "    def func(path: Path) -> torch.Tensor:\n",
    "        return load_and_process_image(path, resize, transforms)\n",
    "\n",
    "    n_workers = cpu_count() or 1\n",
    "    n_workers = 8 if n_workers >= 8 else n_workers\n",
    "\n",
    "    with ThreadPoolExecutor(n_workers) as executor:\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            image_paths_batch = image_paths[i : i + batch_size]\n",
    "            images = list(\n",
    "                executor.map(\n",
    "                    func,\n",
    "                    image_paths_batch,\n",
    "                )\n",
    "            )\n",
    "            yield torch.stack(images).to(device), image_paths_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_mask(mask: torch.Tensor, image_path: Path) -> None:\n",
    "    mask_numpy = mask.squeeze().numpy()\n",
    "    binary_mask = (mask_numpy > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Filter out small segments\n",
    "    filtered_binary_mask = filter_small_segments(binary_mask, min_area=64**2)\n",
    "\n",
    "    output_file_path = OUTPUT_DIR / f'{image_path.stem}.png'\n",
    "    cv2.imwrite(str(output_file_path), filtered_binary_mask * 255)\n",
    "    assert output_file_path.exists(), f'Failed to save mask to `{output_file_path}`.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the device (GPU/CPU)\n",
    "model_ = model_.to(device)\n",
    "\n",
    "image_paths = sorted(INPUT_DIR.glob('*.jpg'))\n",
    "loader = data_loader(image_paths, BATCH_SIZE, (256, 256), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~135 FPS\n",
    "\n",
    "executor = ThreadPoolExecutor(4)\n",
    "for images, image_paths in tqdm(loader, total=len(image_paths) // BATCH_SIZE):\n",
    "    with torch.no_grad():\n",
    "        prediction = model_(images)\n",
    "        masks = prediction.sigmoid()\n",
    "\n",
    "    for mask, image_path in zip(masks, image_paths):\n",
    "        executor.submit(process_and_save_mask, mask.to('cpu'), image_path)\n",
    "\n",
    "executor.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIR / 'readme.txt', 'w') as f:\n",
    "    f.write(f'Timestamp: {datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")}\\n')\n",
    "    f.write(f'Input path: {INPUT_DIR}\\n')\n",
    "    f.write(f'Model: {model_name} v{model_version}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_images_to_mp4(OUTPUT_DIR, OUTPUT_DIR / 'masks.mp4', fps=30, image_format='png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
