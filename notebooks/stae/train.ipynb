{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver State Anomaly Detection With STAE\n",
    "\n",
    "[https://dagshub.com/matejfric/driver-state](https://dagshub.com/matejfric/driver-state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import dagshub\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchview\n",
    "from mlflow.models.signature import infer_signature\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "# Pytorch Lightning EarlyStopping callback does not recover the best weights as in Keras!\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# https://github.com/Lightning-AI/pytorch-lightning/discussions/10399,\n",
    "# https://pytorch-lightning.readthedocs.io/en/1.5.10/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from model.ae import summarize_model\n",
    "from model.ae.temporal_3d import RegularizationType, STAEModel\n",
    "from model.common import Anomalies, BatchSizeDict\n",
    "from model.dataset import STAEDataset\n",
    "from model.eval import compute_best_roc_auc\n",
    "from model.fonts import set_cmu_serif_font\n",
    "from model.git import get_commit_id, get_current_branch\n",
    "from model.logging import (\n",
    "    get_early_stopping_epoch,\n",
    "    get_submodule_param_count,\n",
    "    log_dict_to_mlflow,\n",
    ")\n",
    "from model.plot import (\n",
    "    plot_error_and_anomalies,\n",
    "    plot_learning_curves,\n",
    "    plot_pr_chart,\n",
    "    plot_roc_chart,\n",
    "    plot_stae_reconstruction,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "set_cmu_serif_font()\n",
    "\n",
    "# Experiment logging\n",
    "REPO_NAME = 'driver-stae'\n",
    "USER_NAME = 'matejfric'\n",
    "dagshub.init(REPO_NAME, USER_NAME, mlflow=True)  # type: ignore\n",
    "\n",
    "# Reproducibility\n",
    "# https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility\n",
    "SEED = 42\n",
    "L.seed_everything(SEED, workers=True)\n",
    "\n",
    "print(\n",
    "    f'torch: {torch.__version__}, cuda: {torch.cuda.is_available()}, lightning: {L.__version__}'  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "driver = None\n",
    "source_type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "# ----------------------------------------\n",
    "MAX_EPOCHS = 50\n",
    "MONITOR = 'valid_total_loss'\n",
    "PATIENCE = 7\n",
    "# Run memory map script to use different image size (`run_memory_map_conversion.py`)\n",
    "IMAGE_SIZE: Literal[64, 128, 224, 256] = 64\n",
    "BATCH_SIZE = 64\n",
    "SEQUENCE_LENGTH = 16\n",
    "TIME_STEP = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "LAMBDA_REG = 1e-4\n",
    "TRAIN_SET_RATIO = 0.9\n",
    "USE_MASK = True\n",
    "USE_2D_BOTTLENECK: list[int] | None = [64, 128]\n",
    "USE_EXTRA_3DCONV = True\n",
    "REGULARIZATION: RegularizationType | None = 'l2_model_weights'\n",
    "USE_PREDICTION_BRANCH = True\n",
    "SOURCE_TYPE = source_type or 'depth'\n",
    "\n",
    "# LOGGING\n",
    "# ----------------------------------------\n",
    "DRIVER_MAP = {\n",
    "    'geordi': '2021_08_31_geordi_enyaq',\n",
    "    'poli': '2021_09_06_poli_enyaq',\n",
    "    'michal': '2021_11_05_michal_enyaq',\n",
    "    'dans': '2021_11_18_dans_enyaq',\n",
    "    'jakub': '2021_11_18_jakubh_enyaq',\n",
    "}\n",
    "DRIVER = driver or 'geordi'\n",
    "LOG_DIR = Path('logs')\n",
    "EXPERIMENT_NAME = f'{datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")}-{DRIVER}'\n",
    "VERSION = 0\n",
    "EXPERIMENT_DIR = LOG_DIR / EXPERIMENT_NAME / f'version_{VERSION}'\n",
    "DATASET_NAME = f'2024-10-28-driver-all-frames/{DRIVER_MAP[DRIVER]}'\n",
    "\n",
    "MLFLOW_ARTIFACT_DIR = 'outputs'\n",
    "METRICS_CSV_NAME = 'metrics.csv'\n",
    "LEARNING_CURVES_PDF_NAME = 'learning_curves.pdf'\n",
    "PREDICTIONS_PNG_NAME = 'predictions.png'\n",
    "TRAIN_TRANSFORMS_JSON_NAME = 'train_transforms.json'\n",
    "NOTEBOOK_NAME = 'train.ipynb'\n",
    "ARCHITECTURE_VISUALIZATION_NAME = 'architecture'\n",
    "MODEL_SUMMARY_NAME = 'model_summary.txt'\n",
    "ROC_CHART_NAME = 'roc_chart.pdf'\n",
    "MODEL_ONNX_NAME = 'model.onnx'\n",
    "ERROR_CHART_NAME = 'error_chart.pdf'\n",
    "PR_CHART_NAME = 'pr_chart.pdf'\n",
    "PREDICTIONS_JSON_NAME = 'predictions.json'\n",
    "\n",
    "# DATASET\n",
    "# ----------------------------------------\n",
    "DATASET_DIR = Path().home() / f'source/driver-dataset/{DATASET_NAME}'\n",
    "\n",
    "memory_map_filename = f'{SOURCE_TYPE}_{IMAGE_SIZE}{\"\" if USE_MASK else \"_no_mask\"}.dat'\n",
    "NORMAL_MEMORY_MAP = DATASET_DIR / 'normal' / 'memory_maps' / memory_map_filename\n",
    "ANOMAL_MEMORY_MAP = DATASET_DIR / 'anomal' / 'memory_maps' / memory_map_filename\n",
    "ANOMALIES_FILE = DATASET_DIR / 'anomal' / 'labels.txt'\n",
    "\n",
    "assert NORMAL_MEMORY_MAP.exists(), (\n",
    "    f'Normal memory map does not exist: {NORMAL_MEMORY_MAP}'\n",
    ")\n",
    "assert ANOMAL_MEMORY_MAP.exists(), (\n",
    "    f'Anomal memory map does not exist: {ANOMAL_MEMORY_MAP}'\n",
    ")\n",
    "assert ANOMALIES_FILE.exists(), f'Anomalies file does not exist: {ANOMALIES_FILE}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_dict = BatchSizeDict(\n",
    "    {'train': BATCH_SIZE, 'valid': BATCH_SIZE, 'test': BATCH_SIZE}\n",
    ")\n",
    "\n",
    "train_val_dataset = STAEDataset(\n",
    "    memory_map_file=NORMAL_MEMORY_MAP,\n",
    "    memory_map_image_shape=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    window_size=SEQUENCE_LENGTH,\n",
    "    time_step=TIME_STEP,\n",
    ")\n",
    "test_dataset = STAEDataset(\n",
    "    memory_map_file=ANOMAL_MEMORY_MAP,\n",
    "    memory_map_image_shape=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    window_size=SEQUENCE_LENGTH,\n",
    "    time_step=TIME_STEP,\n",
    ")\n",
    "\n",
    "# Train validation split\n",
    "train_size = int(TRAIN_SET_RATIO * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size_dict['train'],\n",
    "    shuffle=False,  # YOU DON'T WANT TO SHUFFLE TEMPORAL DATA!\n",
    "    num_workers=int(os.cpu_count()),  # type: ignore\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size_dict['valid'],\n",
    "    shuffle=False,\n",
    "    num_workers=int(os.cpu_count()),  # type: ignore\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # batch_size_dict['test'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # int(os.cpu_count()),  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STAEModel(\n",
    "    batch_size_dict=batch_size_dict,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lambda_reg=LAMBDA_REG,\n",
    "    use_2d_bottleneck=USE_2D_BOTTLENECK,\n",
    "    regularization=REGULARIZATION,\n",
    "    use_prediction_branch=USE_PREDICTION_BRANCH,\n",
    "    use_extra_3dconv=USE_EXTRA_3DCONV,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test input tensor of size (batch_size, channels, time_steps/depth, height, width)\n",
    "INPUT_SAMPLE = torch.randn(BATCH_SIZE, 1, SEQUENCE_LENGTH, IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(INPUT_SAMPLE)\n",
    "reconstruction = output[0]\n",
    "prediction = output[1] if USE_PREDICTION_BRANCH else reconstruction\n",
    "\n",
    "assert INPUT_SAMPLE.shape == reconstruction.shape == prediction.shape, (\n",
    "    f'Input and output shapes do not match! Expected: {INPUT_SAMPLE.shape}, got: {output.shape} and {prediction.shape}'\n",
    ")\n",
    "\n",
    "print(summarize_model(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(LOG_DIR, name=EXPERIMENT_NAME, version=VERSION)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=MONITOR,\n",
    "    mode='min',\n",
    "    patience=PATIENCE,\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=EXPERIMENT_DIR,\n",
    "    filename='{epoch}-{valid_total_loss:3f}',\n",
    "    monitor=MONITOR,\n",
    "    save_top_k=1,  # save only the best model\n",
    "    mode='min',\n",
    ")\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',\n",
    "    logger=csv_logger,\n",
    "    callbacks=[model_checkpoint, early_stopping, progress_bar],\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    log_every_n_steps=1,  # log every batch\n",
    "    # https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility\n",
    "    deterministic=True,\n",
    ")\n",
    "torch.use_deterministic_algorithms(\n",
    "    True, warn_only=True\n",
    ")  # torch 2.5 does not have a deterministic implementation of `max_pool3d_with_indices_backward_cuda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader,\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'Training took {elapsed_time / 60:.2f} minutes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Load from MLflow\n",
    "    model_name = 'pytorch-2024-11-02-073717-tae'\n",
    "    model_version = 1\n",
    "    model_uri = f'models:/{model_name}/{model_version}'\n",
    "    model_ = mlflow.pytorch.load_model(model_uri)\n",
    "else:\n",
    "    model_checkpoint_path = list(EXPERIMENT_DIR.glob('*.ckpt'))[0]\n",
    "    print(f'Loading model from: {model_checkpoint_path}')\n",
    "    model_ = STAEModel.load_from_checkpoint(model_checkpoint_path)\n",
    "model_.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ = L.Trainer(logger=False)  # no need to log anything for validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metrics = trainer_.validate(model_, dataloaders=valid_dataloader, verbose=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = Anomalies.from_file(ANOMALIES_FILE)\n",
    "y_true = anomalies.to_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "errors = defaultdict(list)\n",
    "model_.eval()\n",
    "model_.to(device)\n",
    "for i in tqdm(range(len(test_dataloader))):\n",
    "    input_seq = test_dataloader.dataset[i]['image']\n",
    "    with torch.no_grad():\n",
    "        res = model_(input_seq.unsqueeze(0).to(device))\n",
    "    reconstruction = res[0]  # (B, C, T, W, H)\n",
    "\n",
    "    reconstruction = reconstruction[0].cpu().detach()  # (C, T, W, H)\n",
    "    mse = (\n",
    "        F.mse_loss(reconstruction, input_seq, reduction='none')\n",
    "        .mean(dim=1)\n",
    "        .squeeze()\n",
    "        .mean()\n",
    "        .item()\n",
    "    )\n",
    "    mae = (\n",
    "        F.l1_loss(reconstruction, input_seq, reduction='none')\n",
    "        .mean(dim=1)\n",
    "        .squeeze()\n",
    "        .mean()\n",
    "        .item()\n",
    "    )\n",
    "    fro = (\n",
    "        torch.norm((reconstruction - input_seq).squeeze(), p='fro', dim=0).mean().item()\n",
    "    )\n",
    "\n",
    "    errors['mse'].append(mse)\n",
    "    errors['mae'].append(mae)\n",
    "    errors['fro'].append(fro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = {\n",
    "    'mse': sum(errors['mse']) / len(errors['mse']),\n",
    "    'mae': sum(errors['mae']) / len(errors['mae']),\n",
    "    'fro': sum(errors['fro']) / len(errors['fro']),\n",
    "}\n",
    "for key, value in test_metrics.items():\n",
    "    print(f'{key}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = compute_best_roc_auc(y_true, errors)\n",
    "best_metric = res_dict['best_metric']\n",
    "y_proba = res_dict['y_proba']\n",
    "y_true = y_true[: len(y_proba)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc, optimal_threshold = plot_roc_chart(\n",
    "    y_true,\n",
    "    y_proba,\n",
    "    save_path=EXPERIMENT_DIR / ROC_CHART_NAME,\n",
    "    cbar_text=f'Thresholds ({best_metric.upper()})',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_auc, pr_threshold = plot_pr_chart(\n",
    "    y_true,\n",
    "    y_proba,\n",
    "    save_path=EXPERIMENT_DIR / PR_CHART_NAME,\n",
    "    cbar_text=f'Thresholds ({best_metric.upper()})',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_and_anomalies(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_proba,\n",
    "    threshold=optimal_threshold,\n",
    "    save_path=EXPERIMENT_DIR / ERROR_CHART_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stae_reconstruction(\n",
    "    model_,\n",
    "    test_dataloader,\n",
    "    save_path=EXPERIMENT_DIR / PREDICTIONS_PNG_NAME,\n",
    "    indices=[0] + [anomalies[0].middle()] + [anomalies[2].middle()],\n",
    "    show_heatmap=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(\n",
    "    EXPERIMENT_DIR / METRICS_CSV_NAME,\n",
    "    save_path=EXPERIMENT_DIR / LEARNING_CURVES_PDF_NAME,\n",
    "    metrics={\n",
    "        'mse': 'Mean Squared Error',\n",
    "        'fro': 'Frobenius Norm',\n",
    "        'mae': 'Mean Absolute Error',\n",
    "        'reconstruction_loss': 'Reconstruction Loss',\n",
    "        'prediction_loss': 'Prediction Loss',\n",
    "        'regularization_loss': 'Regularization Loss',\n",
    "    },\n",
    "    figsize=(30, 5),\n",
    "    loss_name='total_loss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "predictions = {\n",
    "    'y_true': y_true,\n",
    "    'y_proba': y_proba,\n",
    "    'errors': errors,\n",
    "    'roc_auc': roc_auc,\n",
    "    'best_metric': best_metric,\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "}\n",
    "with open(EXPERIMENT_DIR / PREDICTIONS_JSON_NAME, 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_visualization = torchview.draw_graph(\n",
    "    model_,\n",
    "    input_size=INPUT_SAMPLE.shape,\n",
    "    graph_dir='TB',\n",
    "    depth=3,\n",
    "    roll=True,\n",
    "    expand_nested=True,\n",
    "    graph_name='Temporal Autoencoder',\n",
    "    save_graph=False,\n",
    "    filename=ARCHITECTURE_VISUALIZATION_NAME,\n",
    "    directory=str(EXPERIMENT_DIR),\n",
    ")\n",
    "architecture_visualization.visual_graph.render(format='svg')\n",
    "architecture_visualization.visual_graph.render(format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.to_onnx(\n",
    "    EXPERIMENT_DIR / MODEL_ONNX_NAME,\n",
    "    INPUT_SAMPLE,\n",
    "    export_params=False,\n",
    "    dynamo=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPERIMENT_DIR / MODEL_SUMMARY_NAME, 'w') as f:\n",
    "    f.write(summarize_model(model_))\n",
    "    f.write('\\n')\n",
    "    f.write(model_.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=f'{EXPERIMENT_NAME}') as run:\n",
    "    try:\n",
    "        mlflow.set_tag('Branch', get_current_branch())\n",
    "        mlflow.set_tag('Commit ID', get_commit_id())\n",
    "        mlflow.set_tag('Dataset', DATASET_NAME)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    mlflow.log_metric('roc_auc', roc_auc)\n",
    "    mlflow.log_metric('pr_auc', pr_auc)\n",
    "    mlflow.log_metric('pr_threshold', pr_threshold)\n",
    "    mlflow.log_metric('optimal_threshold', optimal_threshold)\n",
    "    log_dict_to_mlflow(valid_metrics, 'metric')\n",
    "    log_dict_to_mlflow(test_metrics, 'metric')\n",
    "    log_dict_to_mlflow(get_submodule_param_count(model_), 'param')\n",
    "\n",
    "    mlflow.log_param('batch_size', BATCH_SIZE)\n",
    "    mlflow.log_param('max_epochs', MAX_EPOCHS)\n",
    "    mlflow.log_param('early_stopping', get_early_stopping_epoch(EXPERIMENT_DIR))\n",
    "    mlflow.log_param('monitor', MONITOR)\n",
    "    mlflow.log_param('patience', PATIENCE)\n",
    "    mlflow.log_param('image_size', IMAGE_SIZE)\n",
    "    mlflow.log_param('learning_rate', LEARNING_RATE)\n",
    "    mlflow.log_param('lambda_regularization', LAMBDA_REG)\n",
    "    mlflow.log_param('seed', SEED)\n",
    "    mlflow.log_param('sequence_length', SEQUENCE_LENGTH)\n",
    "    mlflow.log_param('time_step', TIME_STEP)\n",
    "    mlflow.log_param('train_set_ratio', TRAIN_SET_RATIO)\n",
    "    mlflow.log_param('train_sequences', len(train_dataset))\n",
    "    mlflow.log_param('driver', DRIVER)\n",
    "    mlflow.log_param('best_metric', best_metric)\n",
    "    mlflow.log_param('use_mask', USE_MASK)\n",
    "    mlflow.log_param('training_time_s', elapsed_time)\n",
    "    mlflow.log_param('use_2d_bottleneck', USE_2D_BOTTLENECK)\n",
    "    mlflow.log_param('use_extra_3dconv', USE_EXTRA_3DCONV)\n",
    "    mlflow.log_param('regularization', REGULARIZATION)\n",
    "    mlflow.log_param('use_prediction_branch', USE_PREDICTION_BRANCH)\n",
    "    mlflow.log_param('source_type', SOURCE_TYPE)\n",
    "\n",
    "    # CSV metrics, learning curves, predictions, notebook\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / METRICS_CSV_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / LEARNING_CURVES_PDF_NAME), MLFLOW_ARTIFACT_DIR\n",
    "    )\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / PREDICTIONS_PNG_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / ROC_CHART_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / MODEL_SUMMARY_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(Path().cwd() / NOTEBOOK_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / MODEL_ONNX_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / ERROR_CHART_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / PR_CHART_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / PREDICTIONS_JSON_NAME), MLFLOW_ARTIFACT_DIR\n",
    "    )\n",
    "\n",
    "    # Network visualization with `torchview`\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / (ARCHITECTURE_VISUALIZATION_NAME + '.svg')),\n",
    "        MLFLOW_ARTIFACT_DIR,\n",
    "    )\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / (ARCHITECTURE_VISUALIZATION_NAME + '.png')),\n",
    "        MLFLOW_ARTIFACT_DIR,\n",
    "    )\n",
    "\n",
    "    # Models are versioned by default\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model=model_,\n",
    "        artifact_path='model',\n",
    "        registered_model_name=f'pytorch-{EXPERIMENT_NAME}',\n",
    "        signature=infer_signature(\n",
    "            INPUT_SAMPLE.numpy(), INPUT_SAMPLE.numpy(), dict(training=False)\n",
    "        ),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
