{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver State Anomaly Detection\n",
    "\n",
    "[https://dagshub.com/matejfric/driver-state](https://dagshub.com/matejfric/driver-state)\n",
    "\n",
    "TODO: setup a pre-commit hook for https://github.com/mwouts/jupytext and Ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import re\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Literal\n",
    "\n",
    "import albumentations as albu\n",
    "import dagshub\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "# Pytorch Lightning EarlyStopping callback does not recover the best weights as in Keras!\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# https://github.com/Lightning-AI/pytorch-lightning/discussions/10399,\n",
    "# https://pytorch-lightning.readthedocs.io/en/1.5.10/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "from model import (\n",
    "    AutoencoderModel,\n",
    "    DatasetPathsLoader,\n",
    "    DatasetSplit,\n",
    ")\n",
    "from model.ae import ConvDecoder, ConvEncoder, FCDecoder, FCEncoder  # noqa F401\n",
    "from model.augmentation import (\n",
    "    compose_transforms,\n",
    "    hard_transforms,\n",
    "    post_transforms,\n",
    "    pre_transforms,\n",
    ")\n",
    "from model.common import BatchSizeDict\n",
    "from model.git import get_commit_id, get_current_branch\n",
    "from model.plot import (\n",
    "    plot_autoencoder_reconstruction,\n",
    "    plot_learning_curves,\n",
    "    show_examples,  # noqa F401 TODO\n",
    "    show_random,  # noqa F401 TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# Experiment logging\n",
    "REPO_NAME = 'driver-state'\n",
    "USER_NAME = 'matejfric'\n",
    "dagshub.init(REPO_NAME, USER_NAME, mlflow=True)  # type: ignore\n",
    "\n",
    "# Reproducibility\n",
    "# https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility\n",
    "SEED = 42\n",
    "L.seed_everything(SEED, workers=True)\n",
    "\n",
    "print(\n",
    "    f'torch: {torch.__version__}, cuda: {torch.cuda.is_available()}, lightning: {L.__version__}'  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "# ----------------------------------------\n",
    "MAX_EPOCHS = 30\n",
    "MONITOR = 'valid_loss'\n",
    "PATIENCE = 5\n",
    "IMAGE_SIZE = 256  # 224\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "LOSS_FUNCTION = 'mse'  # 'mae'\n",
    "AUGMENTATION = True\n",
    "TRAIN_NOISE_STD_INPUT = 0.0  # 0.025\n",
    "TRAIN_NOISE_STD_LATENT = 0.0  # 0.025\n",
    "HIDDEN_LAYER_SIZE = 256\n",
    "\n",
    "# LOGGING\n",
    "# ----------------------------------------\n",
    "LOG_DIR = Path('logs')\n",
    "EXPERIMENT_NAME = (\n",
    "    f'{datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")}-anomaly-detection-fc-ae'\n",
    ")\n",
    "VERSION = 0\n",
    "EXPERIMENT_DIR = LOG_DIR / EXPERIMENT_NAME / f'version_{VERSION}'\n",
    "DATASET_NAME = '2024-09-15-driver-segmentation-dataset'\n",
    "\n",
    "MLFLOW_ARTIFACT_DIR = 'outputs'\n",
    "METRICS_CSV_NAME = 'metrics.csv'\n",
    "LEARNING_CURVES_PDF_NAME = 'learning_curves.pdf'\n",
    "PREDICTIONS_PNG_NAME = 'predictions.png'\n",
    "TRAIN_TRANSFORMS_JSON_NAME = 'train_transforms.json'\n",
    "NOTEBOOK_NAME = 'anomaly_detection.ipynb'\n",
    "\n",
    "# DATASET\n",
    "# ----------------------------------------\n",
    "DATASET_DIR = Path(f'/home/lanter/source/driver-dataset/{DATASET_NAME}')\n",
    "\n",
    "TRAIN_SET_DIR = 'train'\n",
    "VALID_SET_DIR = 'validation'\n",
    "TEST_SET_DIR = 'test'\n",
    "\n",
    "IMAGES_DIR = 'images'\n",
    "MASKS_DIR = 'masks'\n",
    "DEPTH_DIR = 'depth'\n",
    "\n",
    "# NOTE: We filter out the normal images for training and validation,\n",
    "# and the anomaly images for testing.\n",
    "\n",
    "TRAIN_IMAGES = sorted((DATASET_DIR / TRAIN_SET_DIR / DEPTH_DIR).glob('*normal*.png'))\n",
    "TRAIN_MASKS = sorted((DATASET_DIR / TRAIN_SET_DIR / MASKS_DIR).glob('*normal*.png'))\n",
    "\n",
    "VALID_IMAGES = sorted((DATASET_DIR / VALID_SET_DIR / DEPTH_DIR).glob('*normal*.png'))\n",
    "VALID_MASKS = sorted((DATASET_DIR / VALID_SET_DIR / MASKS_DIR).glob('*normal*.png'))\n",
    "\n",
    "TEST_IMAGES = sorted(\n",
    "    list((DATASET_DIR / TRAIN_SET_DIR / DEPTH_DIR).glob('*anomal*.png'))\n",
    "    + list((DATASET_DIR / VALID_SET_DIR / DEPTH_DIR).glob('*anomal*.png'))\n",
    "    + list((DATASET_DIR / TEST_SET_DIR / DEPTH_DIR).glob('*anomal*.png'))\n",
    ")\n",
    "TEST_MASKS = sorted(\n",
    "    list((DATASET_DIR / TRAIN_SET_DIR / MASKS_DIR).glob('*anomal*.png'))\n",
    "    + list((DATASET_DIR / VALID_SET_DIR / MASKS_DIR).glob('*anomal*.png'))\n",
    "    + list((DATASET_DIR / TEST_SET_DIR / MASKS_DIR).glob('*anomal*.png'))\n",
    ")\n",
    "\n",
    "print(f\"\"\"Train: {(n_train_images:=len(TRAIN_IMAGES)):,}\n",
    "Validation: {(n_valid_images:=len(VALID_IMAGES)):,}\n",
    "Test: {(n_test_images:=len(TEST_IMAGES)):,}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ConvEncoder()\n",
    "decoder = ConvDecoder()\n",
    "\n",
    "# encoder = FCEncoder(IMAGE_SIZE, HIDDEN_LAYER_SIZE)\n",
    "# decoder = FCDecoder(IMAGE_SIZE, HIDDEN_LAYER_SIZE)\n",
    "\n",
    "print(encoder)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test case for forward pass\n",
    "# ---------------------------------\n",
    "\n",
    "# Test input tensor of size (batch_size, channels, height, width)\n",
    "x = torch.randn(BATCH_SIZE, 1, IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "# Forward pass through the encoder and decoder\n",
    "encoded = encoder(x)\n",
    "decoded = decoder(encoded)\n",
    "\n",
    "# Check the shapes\n",
    "print(f'Input shape: {x.shape}')\n",
    "print(f'Encoded shape: {encoded.shape}')\n",
    "print(f'Decoded shape: {decoded.shape}')\n",
    "\n",
    "assert x.shape == decoded.shape, 'Input and output shapes do not match!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude images from the training set\n",
    "# TRAIN_IMAGES = [img for img in TRAIN_IMAGES if 'stribny' not in img.stem]\n",
    "# TRAIN_MASKS = [mask for mask in TRAIN_MASKS if 'stribny' not in mask.stem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_random(TRAIN_IMAGES, TRAIN_MASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_random(VALID_IMAGES, VALID_MASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_random(TEST_IMAGES, TEST_MASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = np.zeros((32, 32))\n",
    "xxx = np.expand_dims(xxx, axis=2)\n",
    "xxx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_transforms = compose_transforms(\n",
    "    [\n",
    "        pre_transforms(image_size=IMAGE_SIZE),\n",
    "        post_transforms(),\n",
    "    ]\n",
    ")\n",
    "if AUGMENTATION:\n",
    "    ae_input_transforms = compose_transforms(\n",
    "        [\n",
    "            pre_transforms(image_size=IMAGE_SIZE),\n",
    "            hard_transforms(),\n",
    "            post_transforms(),\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    ae_input_transforms = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = DatasetPathsLoader(\n",
    "    train=DatasetSplit(images=TRAIN_IMAGES, masks=TRAIN_MASKS),\n",
    "    valid=DatasetSplit(images=VALID_IMAGES, masks=VALID_MASKS),\n",
    "    test=DatasetSplit(images=TEST_IMAGES, masks=TEST_MASKS),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_dict = BatchSizeDict(\n",
    "    {'train': BATCH_SIZE, 'valid': BATCH_SIZE, 'test': BATCH_SIZE}\n",
    ")\n",
    "loaders = dataset_loader.get_loaders(\n",
    "    # set to zero if RuntimeError: Trying to resize storage that is not resizable\n",
    "    dataset='anomaly',\n",
    "    num_workers=int(os.cpu_count()),  # type: ignore\n",
    "    batch_size_dict=batch_size_dict,\n",
    "    train_transforms=default_transforms,\n",
    "    valid_transforms=default_transforms,\n",
    "    test_transforms=default_transforms,\n",
    "    ae_input_transforms=ae_input_transforms,\n",
    ")\n",
    "\n",
    "train_dataloader = loaders['train']\n",
    "valid_dataloader = loaders['valid']\n",
    "test_dataloader = loaders['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoencoderModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    batch_size_dict=batch_size_dict,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    loss_function=LOSS_FUNCTION,\n",
    "    train_noise_std_input=TRAIN_NOISE_STD_INPUT,\n",
    "    train_noise_std_latent=TRAIN_NOISE_STD_LATENT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(LOG_DIR, name=EXPERIMENT_NAME, version=VERSION)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=MONITOR,\n",
    "    mode='min',\n",
    "    patience=PATIENCE,\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=EXPERIMENT_DIR,\n",
    "    filename='{epoch}-{val_loss:3f}',\n",
    "    monitor=MONITOR,\n",
    "    save_top_k=1,  # save only the best model\n",
    "    mode='min',\n",
    ")\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',\n",
    "    logger=csv_logger,\n",
    "    callbacks=[model_checkpoint, early_stopping, progress_bar],\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    log_every_n_steps=1,  # log every batch\n",
    "    # https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility\n",
    "    deterministic=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Load from MLflow\n",
    "    model_name = 'pytorch-2024-10-14-220152-anomaly-detection-conv-ae'\n",
    "    model_version = 1\n",
    "    model_uri = f'models:/{model_name}/{model_version}'\n",
    "    model_ = mlflow.pytorch.load_model(model_uri)\n",
    "else:\n",
    "    model_checkpoint_path = list(EXPERIMENT_DIR.glob('*.ckpt'))[0]\n",
    "    model_ = AutoencoderModel.load_from_checkpoint(\n",
    "        model_checkpoint_path, encoder=encoder, decoder=decoder\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ = L.Trainer(logger=False)  # no need to log anything for validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metrics = trainer_.validate(model_, dataloaders=valid_dataloader, verbose=False)[\n",
    "    0\n",
    "]\n",
    "pprint(valid_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = trainer_.test(model_, dataloaders=test_dataloader, verbose=False)[0]\n",
    "pprint(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_autoencoder_reconstruction(\n",
    "    model_,\n",
    "    test_dataloader,\n",
    "    dataset_path=DATASET_DIR,\n",
    "    save_path=EXPERIMENT_DIR / PREDICTIONS_PNG_NAME,\n",
    "    limit=15,\n",
    "    random_shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(\n",
    "    EXPERIMENT_DIR / METRICS_CSV_NAME,\n",
    "    save_path=EXPERIMENT_DIR / LEARNING_CURVES_PDF_NAME,\n",
    "    metrics={\n",
    "        'mse': 'Mean Squared Error',\n",
    "        'fro': 'Frobenius Norm',\n",
    "        'mae': 'Mean Absolute Error',\n",
    "    },\n",
    "    figsize=(22, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transforms for experiment logging\n",
    "if AUGMENTATION:\n",
    "    albu.save(ae_input_transforms, EXPERIMENT_DIR / TRAIN_TRANSFORMS_JSON_NAME)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_early_stopping_epoch() -> int | None:\n",
    "    checkpoint = list(EXPERIMENT_DIR.glob('*.ckpt'))[0].stem\n",
    "    pattern = r'epoch=(\\d+)'\n",
    "    match = re.search(pattern, checkpoint)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dict_to_mlflow(\n",
    "    dictionary: Mapping[str, int | float], type: Literal['metric', 'param']\n",
    ") -> None:\n",
    "    for k, v in dictionary.items():\n",
    "        if type == 'metric':\n",
    "            mlflow.log_metric(k, v)\n",
    "        elif type == 'param':\n",
    "            mlflow.log_param(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submodule_param_count(model: L.LightningModule) -> dict[str, int]:\n",
    "    \"\"\"Get the number of parameters for each submodule in the model.\"\"\"\n",
    "    param_counts = {}\n",
    "    for name, submodule in model.named_children():\n",
    "        num_params = sum(p.numel() for p in submodule.parameters())\n",
    "        if num_params > 0:\n",
    "            param_counts[f'{name}_parameters'] = num_params\n",
    "    param_counts['total_parameters'] = sum([x for x in param_counts.values()])\n",
    "    return param_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=f'{EXPERIMENT_NAME}') as run:\n",
    "    try:\n",
    "        mlflow.set_tag('Branch', get_current_branch())\n",
    "        mlflow.set_tag('Commit ID', get_commit_id())\n",
    "        mlflow.set_tag('Dataset', DATASET_NAME)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    log_dict_to_mlflow(valid_metrics, 'metric')\n",
    "    log_dict_to_mlflow(test_metrics, 'metric')\n",
    "    log_dict_to_mlflow(get_submodule_param_count(model_), 'param')\n",
    "\n",
    "    mlflow.log_param('encoder', str(encoder))\n",
    "    mlflow.log_param('decoder', str(decoder))\n",
    "    mlflow.log_param('batch_size', BATCH_SIZE)\n",
    "    mlflow.log_param('max_epochs', MAX_EPOCHS)\n",
    "    mlflow.log_param('early_stopping', get_early_stopping_epoch())\n",
    "    mlflow.log_param('monitor', MONITOR)\n",
    "    mlflow.log_param('patience', PATIENCE)\n",
    "    mlflow.log_param('image_size', IMAGE_SIZE)\n",
    "    mlflow.log_param('learning_rate', LEARNING_RATE)\n",
    "    mlflow.log_param('loss_function', LOSS_FUNCTION)\n",
    "    mlflow.log_param('augmentation', AUGMENTATION)\n",
    "    mlflow.log_param('train_noise_std_input', TRAIN_NOISE_STD_INPUT)\n",
    "    mlflow.log_param('train_noise_std_latent', TRAIN_NOISE_STD_LATENT)\n",
    "    mlflow.log_param('hidden_layer_size', HIDDEN_LAYER_SIZE)\n",
    "    mlflow.log_param('seed', SEED)\n",
    "    mlflow.log_param('n_train_images', n_train_images)\n",
    "    mlflow.log_param('n_valid_images', n_valid_images)\n",
    "    mlflow.log_param('n_test_images', n_test_images)\n",
    "\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / METRICS_CSV_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / LEARNING_CURVES_PDF_NAME), MLFLOW_ARTIFACT_DIR\n",
    "    )\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / PREDICTIONS_PNG_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    if AUGMENTATION:\n",
    "        mlflow.log_artifact(\n",
    "            str(EXPERIMENT_DIR / TRAIN_TRANSFORMS_JSON_NAME), MLFLOW_ARTIFACT_DIR\n",
    "        )\n",
    "    mlflow.log_artifact(NOTEBOOK_NAME, MLFLOW_ARTIFACT_DIR)\n",
    "\n",
    "    input = np.random.random((BATCH_SIZE, 1, IMAGE_SIZE, IMAGE_SIZE))\n",
    "    signature = mlflow.models.infer_signature(input, input, dict(training=False))  # type: ignore\n",
    "\n",
    "    # Models are versioned by default\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model=model_,\n",
    "        artifact_path='model',\n",
    "        registered_model_name=f'pytorch-{EXPERIMENT_NAME}',\n",
    "        signature=signature,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
