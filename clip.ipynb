{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import clip\n",
    "import dagshub\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model.common import Anomalies, crop_driver_image_contains\n",
    "from model.git import get_commit_id, get_current_branch\n",
    "from model.plot import plot_roc_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "driver = 'geordi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# Experiment logging\n",
    "REPO_NAME = 'driver-state'\n",
    "USER_NAME = 'matejfric'\n",
    "dagshub.init(REPO_NAME, USER_NAME, mlflow=True)  # type: ignore\n",
    "\n",
    "DRIVER_MAP = {\n",
    "    'geordi': '2021_08_31_geordi_enyaq',\n",
    "    'poli': '2021_09_06_poli_enyaq',\n",
    "    'michal': '2021_11_05_michal_enyaq',\n",
    "    'dans': '2021_11_18_dans_enyaq',\n",
    "    'jakub': '2021_11_18_jakubh_enyaq',\n",
    "}\n",
    "DRIVER = driver\n",
    "DATASET_NAME = f'2024-10-28-driver-all-frames/{DRIVER_MAP[DRIVER]}'\n",
    "DATASET_DIR = Path().home() / f'source/driver-dataset/{DATASET_NAME}'\n",
    "\n",
    "ANOMALIES_FILE = DATASET_DIR / 'anomal' / 'labels.txt'\n",
    "assert ANOMALIES_FILE.exists(), f'Anomalies file does not exist: {ANOMALIES_FILE}'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "PREFIX = 'a photo of a person inside a car'\n",
    "PROMPTS = {\n",
    "    'normal': f'{PREFIX} with both hands on the steering wheel',\n",
    "    'anomal': f'{PREFIX} coughing, scratching, or holding a phone',\n",
    "}\n",
    "pprint(PROMPTS)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "ANOMAL_IMAGES_PATHS = sorted((DATASET_DIR / 'anomal/images/').glob('*.jpg'))\n",
    "\n",
    "# LOGGING\n",
    "# ----------\n",
    "NOTEBOOK_NAME = 'clip.ipynb'\n",
    "PREDS_JSON_NAME = 'preds.json'\n",
    "PROMPTS_JSON_NAME = 'prompts.json'\n",
    "MLFLOW_ARTIFACT_DIR = 'outputs'\n",
    "MODEL_NAME = 'CLIP'\n",
    "LOG_DIR = Path('logs')\n",
    "EXPERIMENT_NAME = (\n",
    "    f'{datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")}-{MODEL_NAME}-{DRIVER}'\n",
    ")\n",
    "VERSION = 0\n",
    "EXPERIMENT_DIR = LOG_DIR / EXPERIMENT_NAME / f'version_{VERSION}'\n",
    "ROC_CHART_NAME = 'roc_chart.svg'\n",
    "EXPERIMENT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "text = clip.tokenize(list(PROMPTS.values())).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "y_pred = []\n",
    "y_pred_proba = []\n",
    "\n",
    "\n",
    "# Function to preprocess a single image\n",
    "def preprocess_image(image_path: Path) -> torch.Tensor:\n",
    "    \"\"\"Square crop and resize the image to 224x224.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = crop_driver_image_contains(image, image_path)\n",
    "    # The `preprocess` function resizes to 224x224\n",
    "    # and normalizes the image.\n",
    "    return preprocess(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split ANOMAL_IMAGES into batches\n",
    "with tqdm(total=len(ANOMAL_IMAGES_PATHS), desc='Processing') as pbar:\n",
    "    for i in range(0, len(ANOMAL_IMAGES_PATHS), BATCH_SIZE):\n",
    "        batch = ANOMAL_IMAGES_PATHS[i : i + BATCH_SIZE]\n",
    "\n",
    "        # Use ThreadPoolExecutor to preprocess images in parallel\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            processed_images = list(executor.map(preprocess_image, batch))\n",
    "\n",
    "        # Stack and move the preprocessed images to the device\n",
    "        images = torch.stack(processed_images).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Encode image batch\n",
    "            image_features = model.encode_image(images)\n",
    "\n",
    "            # Pass the image batch and text batch to the model\n",
    "            logits_per_image, logits_per_text = model(\n",
    "                images, text\n",
    "            )  # Assuming text is preprocessed and batched if needed\n",
    "            proba_batch = logits_per_image.softmax(dim=-1)\n",
    "            cls_batch = proba_batch.argmax(dim=-1).cpu().detach().tolist()\n",
    "        proba_batch = proba_batch.cpu().detach().tolist()\n",
    "\n",
    "        y_pred_proba.extend(proba_batch)  # Append batch probabilities\n",
    "        y_pred.extend(cls_batch)  # Append batch predictions\n",
    "        pbar.update(len(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = Anomalies.from_file(ANOMALIES_FILE)\n",
    "y_true = anomalies.to_ground_truth(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc, optimal_threshold = plot_roc_chart(\n",
    "    y_true,\n",
    "    np.array(y_pred_proba)[:, 1],\n",
    "    save_path=EXPERIMENT_DIR / ROC_CHART_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPERIMENT_DIR / PREDS_JSON_NAME, 'w') as f:\n",
    "    json.dump(y_pred_proba, f)\n",
    "\n",
    "with open(EXPERIMENT_DIR / PROMPTS_JSON_NAME, 'w') as f:\n",
    "    json.dump(PROMPTS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=f'{EXPERIMENT_NAME}') as run:\n",
    "    try:\n",
    "        mlflow.set_tag('Branch', get_current_branch())\n",
    "        mlflow.set_tag('Commit ID', get_commit_id())\n",
    "        mlflow.set_tag('Dataset', DATASET_NAME)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    mlflow.log_metric('roc_auc', roc_auc)\n",
    "    mlflow.log_metric('optimal_threshold', optimal_threshold)\n",
    "    mlflow.log_param('driver', DRIVER)\n",
    "    mlflow.log_param('model', MODEL_NAME)\n",
    "    mlflow.log_param('prompts', PROMPTS)\n",
    "\n",
    "    # For comparison with the previous experiments\n",
    "    mlflow.log_param('sequence_length', 1)\n",
    "    mlflow.log_param('time_step', 1)\n",
    "    mlflow.log_param('image_size', 224)\n",
    "    mlflow.log_param('use_mask', False)\n",
    "\n",
    "    # Artifacts\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / PREDS_JSON_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / PROMPTS_JSON_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(NOTEBOOK_NAME, MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / ROC_CHART_NAME), MLFLOW_ARTIFACT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
