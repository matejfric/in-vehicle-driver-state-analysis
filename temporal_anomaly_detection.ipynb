{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver State Anomaly Detection With Temporal Autoencoders\n",
    "\n",
    "[https://dagshub.com/matejfric/driver-state](https://dagshub.com/matejfric/driver-state)\n",
    "\n",
    "TODO: setup a pre-commit hook for https://github.com/mwouts/jupytext and Ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import dagshub\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import numpy as np\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "import torchview\n",
    "from mlflow.models.signature import infer_signature\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "# Pytorch Lightning EarlyStopping callback does not recover the best weights as in Keras!\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# https://github.com/Lightning-AI/pytorch-lightning/discussions/10399,\n",
    "# https://pytorch-lightning.readthedocs.io/en/1.5.10/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model.ae import (  # noqa F401\n",
    "    LSTMDecoder,\n",
    "    LSTMEncoder,\n",
    "    TemporalAutoencoderModel,\n",
    "    summarize_model,\n",
    ")\n",
    "from model.ae.iscv2023 import (  # noqa F401\n",
    "    EfficientNetEncoder,\n",
    "    ISVC23DecoderV1,\n",
    "    ISVC23DecoderV2,\n",
    "    ISVC23DecoderV3,\n",
    "    ISVC23DecoderV4,\n",
    "    ISVC23EncoderV1,\n",
    "    ISVC23EncoderV2,\n",
    "    ISVC23EncoderV3,\n",
    "    ISVC23EncoderV4,\n",
    ")\n",
    "from model.ae.temporal_3d import (\n",
    "    Conv3dDecoder,\n",
    "    Conv3dEncoder,\n",
    ")\n",
    "from model.common import Anomalies, BatchSizeDict\n",
    "from model.dataset import TemporalAutoencoderDataset\n",
    "from model.eval import get_y_proba_from_errors\n",
    "from model.git import get_commit_id, get_current_branch\n",
    "from model.logging import (\n",
    "    get_early_stopping_epoch,\n",
    "    get_experiment_id,\n",
    "    get_submodule_param_count,\n",
    "    log_dict_to_mlflow,\n",
    ")\n",
    "from model.plot import (\n",
    "    plot_error_and_anomalies,\n",
    "    plot_learning_curves,\n",
    "    plot_pr_chart,\n",
    "    plot_roc_chart,\n",
    "    plot_temporal_autoencoder_reconstruction,\n",
    "    show_examples,  # noqa F401 TODO\n",
    "    show_random,  # noqa F401 TODO\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# Experiment logging\n",
    "REPO_NAME = 'driver-tae'\n",
    "USER_NAME = 'matejfric'\n",
    "dagshub.init(REPO_NAME, USER_NAME, mlflow=True)  # type: ignore\n",
    "\n",
    "# Reproducibility\n",
    "# https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility\n",
    "SEED = 42\n",
    "L.seed_everything(SEED, workers=True)\n",
    "\n",
    "print(\n",
    "    f'torch: {torch.__version__}, cuda: {torch.cuda.is_available()}, lightning: {L.__version__}'  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "driver = 'geordi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "# ----------------------------------------\n",
    "MAX_EPOCHS = 10\n",
    "MONITOR = 'valid_loss'\n",
    "PATIENCE = 5\n",
    "# Run memory map script to use a different image size (`run_memory_map_conversion.py`)\n",
    "IMAGE_SIZE: Literal[64, 128, 224, 256] = 64\n",
    "BATCH_SIZE = 128\n",
    "SEQUENCE_LENGTH = 2\n",
    "TIME_STEP = 1\n",
    "LEARNING_RATE = 0.0005  # 1e-4\n",
    "LOSS_FUNCTION = 'mse'  # 'mae'\n",
    "TRAIN_SET_RATIO = 0.9\n",
    "USE_MASK = True\n",
    "MODEL_NAME: Literal['tae', 'tae3d'] = 'tae'\n",
    "LATENT_DIM = 128\n",
    "\n",
    "# LOGGING\n",
    "# ----------------------------------------\n",
    "DRIVER_MAP = {\n",
    "    'geordi': '2021_08_31_geordi_enyaq',\n",
    "    'poli': '2021_09_06_poli_enyaq',\n",
    "    'michal': '2021_11_05_michal_enyaq',\n",
    "    'dans': '2021_11_18_dans_enyaq',\n",
    "    'jakub': '2021_11_18_jakubh_enyaq',\n",
    "    'radovan': '2024_07_02_radovan_enyaq',\n",
    "}\n",
    "NOW = datetime.datetime.now().strftime('%Y-%m-%d-%H%M%S')\n",
    "DRIVER = driver\n",
    "EXPERIMENT_NAME = driver\n",
    "LOG_DIR = Path('logs')\n",
    "RUN_NAME = f'{MODEL_NAME}-binary-mask-test'\n",
    "VERSION = 0\n",
    "EXPERIMENT_DIR = LOG_DIR / f'{NOW}-{RUN_NAME}' / f'version_{VERSION}'\n",
    "DATASET_NAME = f'2024-10-28-driver-all-frames/{DRIVER_MAP[DRIVER]}'\n",
    "\n",
    "MLFLOW_ARTIFACT_DIR = 'outputs'\n",
    "METRICS_CSV_NAME = 'metrics.csv'\n",
    "LEARNING_CURVES_PDF_NAME = 'learning_curves.pdf'\n",
    "PREDICTIONS_PNG_NAME = 'predictions.png'\n",
    "PREDICTIONS_JSON_NAME = 'predictions.json'\n",
    "TRAIN_TRANSFORMS_JSON_NAME = 'train_transforms.json'\n",
    "NOTEBOOK_NAME = 'temporal_anomaly_detection.ipynb'\n",
    "ARCHITECTURE_VISUALIZATION_NAME = 'architecture'\n",
    "MODEL_SUMMARY_NAME = 'model_summary.txt'\n",
    "ROC_CHART_NAME = 'roc_chart.pdf'\n",
    "ERROR_CHART_NAME = 'error_chart.pdf'\n",
    "PR_CHART_NAME = 'pr_chart.pdf'\n",
    "SOURCE_TYPE: Literal['depth', 'video_depth_anything', 'depth_realsense', 'masks'] = (\n",
    "    'masks'\n",
    ")\n",
    "TEST_SESSION = 'anomal'  # ~, 181149, 182201\n",
    "# MODEL_ONNX_NAME = 'model.onnx'\n",
    "\n",
    "# DATASET\n",
    "# ----------------------------------------\n",
    "DATASET_DIR = Path().home() / f'source/driver-dataset/{DATASET_NAME}'\n",
    "\n",
    "memory_map_filename = f'{SOURCE_TYPE}_{IMAGE_SIZE}{\"\" if USE_MASK else \"_no_mask\"}.dat'\n",
    "NORMAL_MEMORY_MAP = DATASET_DIR / 'normal' / 'memory_maps' / memory_map_filename\n",
    "ANOMAL_MEMORY_MAP = DATASET_DIR / TEST_SESSION / 'memory_maps' / memory_map_filename\n",
    "ANOMALIES_FILE = DATASET_DIR / TEST_SESSION / 'labels.txt'\n",
    "\n",
    "assert NORMAL_MEMORY_MAP.exists(), (\n",
    "    f'Normal memory map does not exist: {NORMAL_MEMORY_MAP}'\n",
    ")\n",
    "assert ANOMAL_MEMORY_MAP.exists(), (\n",
    "    f'Anomal memory map does not exist: {ANOMAL_MEMORY_MAP}'\n",
    ")\n",
    "assert ANOMALIES_FILE.exists(), f'Anomalies file does not exist: {ANOMALIES_FILE}'\n",
    "\n",
    "if MODEL_NAME == 'tae3d':\n",
    "    assert SEQUENCE_LENGTH >= 16, (\n",
    "        'Number of time steps must be at least 16 for the 3D convolution model'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test case for forward pass, also used as model signature in Mlflow.\n",
    "\n",
    "if MODEL_NAME == 'tae':\n",
    "    # encoder = LSTMEncoder(n_time_steps=SEQUENCE_LENGTH, bidirectional=True)\n",
    "    # decoder = LSTMDecoder(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH,\n",
    "    #     n_image_channels=1,\n",
    "    #     image_size=IMAGE_SIZE,\n",
    "    #     bidirectional=True,\n",
    "    # )\n",
    "\n",
    "    # encoder = EfficientNetEncoder(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH,\n",
    "    #     bidirectional=True,\n",
    "    #     image_size=IMAGE_SIZE,\n",
    "    #     latent_dim=LATENT_DIM,\n",
    "    # )\n",
    "\n",
    "    encoder = ISVC23EncoderV1(\n",
    "        n_time_steps=SEQUENCE_LENGTH,\n",
    "        bidirectional=True,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        latent_dim=LATENT_DIM,\n",
    "    )\n",
    "    decoder = ISVC23DecoderV1(\n",
    "        n_time_steps=SEQUENCE_LENGTH,\n",
    "        bidirectional=True,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        latent_dim=LATENT_DIM,\n",
    "    )\n",
    "\n",
    "    # encoder = ISVC23EncoderV2(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH, image_size=IMAGE_SIZE, latent_dim=LATENT_DIM\n",
    "    # )\n",
    "    # decoder = ISVC23DecoderV2(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH, image_size=IMAGE_SIZE, latent_dim=LATENT_DIM\n",
    "    # )\n",
    "\n",
    "    # encoder = ISVC23EncoderV3(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH, image_size=IMAGE_SIZE, latent_dim=LATENT_DIM\n",
    "    # )\n",
    "    # decoder = ISVC23DecoderV3(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH, image_size=IMAGE_SIZE, latent_dim=LATENT_DIM\n",
    "    # )\n",
    "\n",
    "    # encoder = ISVC23EncoderV4(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH, image_size=IMAGE_SIZE, latent_dim=LATENT_DIM\n",
    "    # )\n",
    "    # decoder = ISVC23DecoderV4(\n",
    "    #     n_time_steps=SEQUENCE_LENGTH, image_size=IMAGE_SIZE, latent_dim=LATENT_DIM\n",
    "    # )\n",
    "\n",
    "    # Test input tensor of size (batch_size, time_steps, channels, height, width)\n",
    "    INPUT_SAMPLE = torch.randn(BATCH_SIZE, SEQUENCE_LENGTH, 1, IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "    # Forward pass through the encoder and decoder\n",
    "    encoded = encoder(INPUT_SAMPLE)\n",
    "    decoded = decoder(encoded)\n",
    "\n",
    "    # Check the shapes\n",
    "    print(f'Input shape: {INPUT_SAMPLE.shape}')\n",
    "    print(f'Latent shape: {encoded.shape}')\n",
    "    print(f'Decoded shape: {decoded.shape}')\n",
    "\n",
    "    assert INPUT_SAMPLE.shape == decoded.shape, 'Input and output shapes do not match!'\n",
    "\n",
    "    print(summarize_model([encoder, decoder]))\n",
    "\n",
    "    # torchinfo.summary(\n",
    "    #     encoder,\n",
    "    #     input_size=(BATCH_SIZE, SEQUENCE_LENGTH, 1, IMAGE_SIZE, IMAGE_SIZE),\n",
    "    #     depth=4,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test case for forward pass, also used as model signature in Mlflow.\n",
    "\n",
    "if MODEL_NAME == 'tae3d':\n",
    "    encoder = Conv3dEncoder()\n",
    "    decoder = Conv3dDecoder()\n",
    "\n",
    "    # Test input tensor of size (batch_size, channels, time_steps/depth, height, width)\n",
    "    INPUT_SAMPLE = torch.randn(BATCH_SIZE, 1, SEQUENCE_LENGTH, IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "    # Forward pass through the encoder and decoder\n",
    "    encoded = encoder(INPUT_SAMPLE)\n",
    "    decoded = decoder(encoded)\n",
    "\n",
    "    # Check the shapes\n",
    "    print(f'Input shape: {INPUT_SAMPLE.shape}')\n",
    "    print(f'Latent shape: {encoded.shape}')\n",
    "    print(f'Decoded shape: {decoded.shape}')\n",
    "\n",
    "    assert INPUT_SAMPLE.shape == decoded.shape, 'Input and output shapes do not match!'\n",
    "\n",
    "    print(summarize_model([encoder, decoder]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_DIM_INDEX = 0 if MODEL_NAME == 'tae' else 1\n",
    "DTYPE = np.float32 if SOURCE_TYPE == 'depth_realsense' else np.uint8\n",
    "\n",
    "batch_size_dict = BatchSizeDict(\n",
    "    {'train': BATCH_SIZE, 'valid': BATCH_SIZE, 'test': BATCH_SIZE}\n",
    ")\n",
    "\n",
    "train_val_dataset = TemporalAutoencoderDataset(\n",
    "    memory_map_file=NORMAL_MEMORY_MAP,\n",
    "    memory_map_image_shape=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    window_size=SEQUENCE_LENGTH,\n",
    "    time_step=TIME_STEP,\n",
    "    time_dim_index=TIME_DIM_INDEX,\n",
    "    dtype=DTYPE,\n",
    ")\n",
    "test_dataset = TemporalAutoencoderDataset(\n",
    "    memory_map_file=ANOMAL_MEMORY_MAP,\n",
    "    memory_map_image_shape=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    window_size=SEQUENCE_LENGTH,\n",
    "    time_step=TIME_STEP,\n",
    "    time_dim_index=TIME_DIM_INDEX,\n",
    "    dtype=DTYPE,\n",
    ")\n",
    "\n",
    "# Train validation split\n",
    "train_size = int(TRAIN_SET_RATIO * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size_dict['train'],\n",
    "    shuffle=False,  # YOU DON'T WANT TO SHUFFLE TEMPORAL DATA!\n",
    "    num_workers=int(os.cpu_count()),  # type: ignore\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size_dict['valid'],\n",
    "    shuffle=False,\n",
    "    num_workers=int(os.cpu_count()),  # type: ignore\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # batch_size_dict['test'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # int(os.cpu_count()),  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalAutoencoderModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    batch_size_dict=batch_size_dict,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    loss_function=LOSS_FUNCTION,\n",
    "    time_dim_index=1 if MODEL_NAME == 'tae' else 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(LOG_DIR, name=f'{NOW}-{RUN_NAME}', version=VERSION)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=MONITOR,\n",
    "    mode='min',\n",
    "    patience=PATIENCE,\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=EXPERIMENT_DIR,\n",
    "    filename='{epoch}-{valid_loss:3f}',\n",
    "    monitor=MONITOR,\n",
    "    save_top_k=1,  # save only the best model\n",
    "    mode='min',\n",
    ")\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',\n",
    "    logger=csv_logger,\n",
    "    callbacks=[model_checkpoint, early_stopping, progress_bar],\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    log_every_n_steps=1,  # log every batch\n",
    "    # https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility\n",
    "    deterministic=True,\n",
    ")\n",
    "torch.use_deterministic_algorithms(\n",
    "    True, warn_only=True\n",
    ")  # torch 2.5 does not have a deterministic implementation of `max_pool3d_with_indices_backward_cuda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Load from MLflow\n",
    "    model_name = 'pytorch-2025-02-16-201321-tae-radovan-isvc23v1-L515-test'\n",
    "    model_version = 1\n",
    "    model_uri = f'models:/{model_name}/{model_version}'\n",
    "    model_ = mlflow.pytorch.load_model(model_uri)\n",
    "else:\n",
    "    model_checkpoint_path = list(EXPERIMENT_DIR.glob('*.ckpt'))[0]\n",
    "    print(f'Loading model from: {model_checkpoint_path}')\n",
    "    model_ = TemporalAutoencoderModel.load_from_checkpoint(\n",
    "        model_checkpoint_path, encoder=encoder, decoder=decoder\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ = L.Trainer(logger=False)  # no need to log anything for validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metrics = trainer_.validate(model_, dataloaders=valid_dataloader, verbose=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_metrics = trainer_.test(model_, dataloaders=test_dataloader, verbose=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = Anomalies.from_file(ANOMALIES_FILE)\n",
    "n_test_frames = len(list((ANOMALIES_FILE.parent / 'images').glob('*.jpg')))\n",
    "y_true = anomalies.to_ground_truth(n_test_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "errors = defaultdict(list)\n",
    "model_.eval()\n",
    "model_.to(device)\n",
    "for i in tqdm(range(len(test_dataloader))):\n",
    "    input_seq = test_dataloader.dataset[i]['image']\n",
    "    with torch.no_grad():\n",
    "        res = model_(input_seq.unsqueeze(0).to(device))[0]\n",
    "\n",
    "    res = res.cpu()\n",
    "    mse = (\n",
    "        F.mse_loss(res, input_seq, reduction='none').mean(dim=1).squeeze().mean().item()\n",
    "    )\n",
    "    mae = (\n",
    "        F.l1_loss(res, input_seq, reduction='none').mean(dim=1).squeeze().mean().item()\n",
    "    )\n",
    "    fro = torch.norm((res - input_seq).squeeze(), p='fro', dim=0).mean().item()\n",
    "\n",
    "    errors['mse'].append(mse)\n",
    "    errors['mae'].append(mae)\n",
    "    errors['fro'].append(fro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = {\n",
    "    'mse': sum(errors['mse']) / len(errors['mse']),\n",
    "    'mae': sum(errors['mae']) / len(errors['mae']),\n",
    "    'fro': sum(errors['fro']) / len(errors['fro']),\n",
    "}\n",
    "for key, value in test_metrics.items():\n",
    "    print(f'{key}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_true), len(errors['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ROC AUC score for each metric, choose the best one.\n",
    "best_metric = 'mse'\n",
    "best_roc_auc_score = 0.0\n",
    "for key, value in errors.items():\n",
    "    y_proba = get_y_proba_from_errors(value)\n",
    "    if len(y_proba) != len(y_true):\n",
    "        print(\n",
    "            f'Ground truth and predictions have different lengths! Truncating from {len(y_true)} to {len(y_proba)}.'\n",
    "        )\n",
    "        y_true = y_true[: len(y_proba)]\n",
    "    roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    if roc_auc > best_roc_auc_score:\n",
    "        best_metric = key\n",
    "        best_roc_auc_score = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = get_y_proba_from_errors(errors[best_metric])\n",
    "roc_auc, optimal_threshold = plot_roc_chart(\n",
    "    y_true=y_true,\n",
    "    y_pred_proba=y_proba,\n",
    "    save_path=EXPERIMENT_DIR / ROC_CHART_NAME,\n",
    "    cbar_text=f'Thresholds ({best_metric.upper()})',\n",
    ")\n",
    "\n",
    "# Save the predictions\n",
    "predictions = {\n",
    "    'y_true': y_true,\n",
    "    'y_proba': y_proba,\n",
    "    'roc_auc': roc_auc,\n",
    "    'best_metric': best_metric,\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "}\n",
    "with open(EXPERIMENT_DIR / PREDICTIONS_JSON_NAME, 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_auc, pr_threshold = plot_pr_chart(\n",
    "    y_true=y_true,\n",
    "    y_pred_proba=y_proba,\n",
    "    save_path=EXPERIMENT_DIR / PR_CHART_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_and_anomalies(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_proba,\n",
    "    threshold=optimal_threshold,\n",
    "    save_path=EXPERIMENT_DIR / ERROR_CHART_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_temporal_autoencoder_reconstruction(\n",
    "    model_,\n",
    "    test_dataloader,\n",
    "    save_path=EXPERIMENT_DIR / PREDICTIONS_PNG_NAME,\n",
    "    time_dim_index=0 if MODEL_NAME == 'tae' else 1,\n",
    "    indices=[0]\n",
    "    + [anomalies[0].middle()]\n",
    "    + [anomalies[1].middle()]\n",
    "    + [anomalies[2].middle()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(\n",
    "    EXPERIMENT_DIR / METRICS_CSV_NAME,\n",
    "    save_path=EXPERIMENT_DIR / LEARNING_CURVES_PDF_NAME,\n",
    "    metrics={\n",
    "        'mse': 'Mean Squared Error',\n",
    "        'fro': 'Frobenius Norm',\n",
    "        'mae': 'Mean Absolute Error',\n",
    "    },\n",
    "    figsize=(22, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_visualization = torchview.draw_graph(\n",
    "    model_,\n",
    "    input_size=INPUT_SAMPLE.shape,\n",
    "    graph_dir='TB',\n",
    "    depth=3,\n",
    "    roll=True,\n",
    "    expand_nested=True,\n",
    "    graph_name='Temporal Autoencoder',\n",
    "    save_graph=False,\n",
    "    filename=ARCHITECTURE_VISUALIZATION_NAME,\n",
    "    directory=str(EXPERIMENT_DIR),\n",
    ")\n",
    "architecture_visualization.visual_graph.render(format='svg')\n",
    "architecture_visualization.visual_graph.render(format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_.to_onnx(\n",
    "#     EXPERIMENT_DIR / MODEL_ONNX_NAME,\n",
    "#     INPUT_SAMPLE,\n",
    "#     export_params=False,\n",
    "#     dynamo=False,\n",
    "#     opset_version=11,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPERIMENT_DIR / MODEL_SUMMARY_NAME, 'w') as f:\n",
    "    f.write(summarize_model([encoder, decoder]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(\n",
    "    run_name=f'{RUN_NAME}', experiment_id=get_experiment_id(EXPERIMENT_NAME)\n",
    ") as run:\n",
    "    try:\n",
    "        mlflow.set_tag('Branch', get_current_branch())\n",
    "        mlflow.set_tag('Commit ID', get_commit_id())\n",
    "        mlflow.set_tag('Dataset', DATASET_NAME)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    mlflow.log_metric('roc_auc', roc_auc)\n",
    "    mlflow.log_metric('optimal_threshold', optimal_threshold)\n",
    "    log_dict_to_mlflow(valid_metrics, 'metric')\n",
    "    log_dict_to_mlflow(test_metrics, 'metric')\n",
    "    log_dict_to_mlflow(get_submodule_param_count(model_), 'param')\n",
    "\n",
    "    mlflow.log_param('encoder', str(encoder))\n",
    "    mlflow.log_param('decoder', str(decoder))\n",
    "    mlflow.log_param('batch_size', BATCH_SIZE)\n",
    "    mlflow.log_param('max_epochs', MAX_EPOCHS)\n",
    "    mlflow.log_param('early_stopping', get_early_stopping_epoch(EXPERIMENT_DIR))\n",
    "    mlflow.log_param('monitor', MONITOR)\n",
    "    mlflow.log_param('patience', PATIENCE)\n",
    "    mlflow.log_param('image_size', IMAGE_SIZE)\n",
    "    mlflow.log_param('learning_rate', LEARNING_RATE)\n",
    "    mlflow.log_param('loss_function', LOSS_FUNCTION)\n",
    "    mlflow.log_param('seed', SEED)\n",
    "    mlflow.log_param('sequence_length', SEQUENCE_LENGTH)\n",
    "    mlflow.log_param('time_step', TIME_STEP)\n",
    "    mlflow.log_param('train_set_ratio', TRAIN_SET_RATIO)\n",
    "    mlflow.log_param('train_sequences', len(train_dataset))\n",
    "    mlflow.log_param('driver', DRIVER)\n",
    "    mlflow.log_param('best_metric', best_metric)\n",
    "    mlflow.log_param('use_mask', USE_MASK)\n",
    "    mlflow.log_param('source_type', SOURCE_TYPE)\n",
    "    mlflow.log_param('latent_dim', LATENT_DIM)\n",
    "    mlflow.log_param('test_session', TEST_SESSION)\n",
    "\n",
    "    # CSV metrics, learning curves, predictions, notebook\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / METRICS_CSV_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / LEARNING_CURVES_PDF_NAME), MLFLOW_ARTIFACT_DIR\n",
    "    )\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / PREDICTIONS_PNG_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / ROC_CHART_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / MODEL_SUMMARY_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / ERROR_CHART_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / PR_CHART_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / PREDICTIONS_JSON_NAME), MLFLOW_ARTIFACT_DIR\n",
    "    )\n",
    "    mlflow.log_artifact(NOTEBOOK_NAME, MLFLOW_ARTIFACT_DIR)\n",
    "\n",
    "    # Network visualization with `torchview`\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / (ARCHITECTURE_VISUALIZATION_NAME + '.svg')),\n",
    "        MLFLOW_ARTIFACT_DIR,\n",
    "    )\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / (ARCHITECTURE_VISUALIZATION_NAME + '.png')),\n",
    "        MLFLOW_ARTIFACT_DIR,\n",
    "    )\n",
    "\n",
    "    # Models are versioned by default\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model=model_,\n",
    "        artifact_path='model',\n",
    "        registered_model_name=f'pytorch-{DRIVER}',\n",
    "        signature=infer_signature(\n",
    "            INPUT_SAMPLE.numpy(), INPUT_SAMPLE.numpy(), dict(training=False)\n",
    "        ),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
