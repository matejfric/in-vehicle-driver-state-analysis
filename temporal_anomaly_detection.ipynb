{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver State Anomaly Detection With Temporal Autoencoders\n",
    "\n",
    "[https://dagshub.com/matejfric/driver-state](https://dagshub.com/matejfric/driver-state)\n",
    "\n",
    "TODO: setup a pre-commit hook for https://github.com/mwouts/jupytext and Ruff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from collections.abc import Mapping\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import dagshub\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchview\n",
    "from mlflow.models.signature import infer_signature\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "# Pytorch Lightning EarlyStopping callback does not recover the best weights as in Keras!\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# https://github.com/Lightning-AI/pytorch-lightning/discussions/10399,\n",
    "# https://pytorch-lightning.readthedocs.io/en/1.5.10/extensions/generated/pytorch_lightning.callbacks.ModelCheckpoint.html\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model.ae import (\n",
    "    LSTMDecoder,\n",
    "    LSTMEncoder,\n",
    "    TemporalAutoencoderModel,\n",
    "    summarize_model,\n",
    ")\n",
    "from model.ae.temporal_3d import (\n",
    "    Conv3dDecoder,\n",
    "    Conv3dEncoder,\n",
    ")\n",
    "from model.common import Anomalies, BatchSizeDict\n",
    "from model.dataset import TemporalAutoencoderDataset\n",
    "from model.git import get_commit_id, get_current_branch\n",
    "from model.plot import (\n",
    "    plot_learning_curves,\n",
    "    plot_roc_chart,\n",
    "    plot_temporal_autoencoder_reconstruction,\n",
    "    show_examples,  # noqa F401 TODO\n",
    "    show_random,  # noqa F401 TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "\n",
    "# rnn = nn.LSTM(128, 20, 1, batch_first=True, bidirectional=True)\n",
    "# input = torch.randn(32, 4, 128)\n",
    "# output, (hn, cn) = rnn(input)\n",
    "# print(output.shape)\n",
    "# print(hn.shape)\n",
    "# print(cn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# Experiment logging\n",
    "REPO_NAME = 'driver-state'\n",
    "USER_NAME = 'matejfric'\n",
    "dagshub.init(REPO_NAME, USER_NAME, mlflow=True)  # type: ignore\n",
    "\n",
    "# Reproducibility\n",
    "# https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility\n",
    "SEED = 42\n",
    "L.seed_everything(SEED, workers=True)\n",
    "\n",
    "print(\n",
    "    f'torch: {torch.__version__}, cuda: {torch.cuda.is_available()}, lightning: {L.__version__}'  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "# ----------------------------------------\n",
    "MAX_EPOCHS = 10\n",
    "MONITOR = 'valid_loss'\n",
    "PATIENCE = 3\n",
    "# Run memory map script to use different image size (`run_memory_map_conversion.py`)\n",
    "IMAGE_SIZE: Literal[128, 224, 256] = 128\n",
    "BATCH_SIZE = 32\n",
    "TIME_STEPS = 2\n",
    "LEARNING_RATE = 0.0005  # 1e-4\n",
    "LOSS_FUNCTION = 'mse'  # 'mae'\n",
    "TRAIN_NOISE_STD_INPUT = 0.0  # 0.025\n",
    "TRAIN_NOISE_STD_LATENT = 0.0  # 0.025\n",
    "TRAIN_SET_RATIO = 0.9\n",
    "\n",
    "# LOGGING\n",
    "# ----------------------------------------\n",
    "MODEL_NAME: Literal['tae', 'tae3d'] = 'tae'\n",
    "LOG_DIR = Path('logs')\n",
    "EXPERIMENT_NAME = f'{datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\")}-{MODEL_NAME}'\n",
    "VERSION = 0\n",
    "EXPERIMENT_DIR = LOG_DIR / EXPERIMENT_NAME / f'version_{VERSION}'\n",
    "DATASET_NAME = '2024-10-28-driver-all-frames/2021_08_31_geordi_enyaq'\n",
    "\n",
    "MLFLOW_ARTIFACT_DIR = 'outputs'\n",
    "METRICS_CSV_NAME = 'metrics.csv'\n",
    "LEARNING_CURVES_PDF_NAME = 'learning_curves.pdf'\n",
    "PREDICTIONS_PNG_NAME = 'predictions.png'\n",
    "TRAIN_TRANSFORMS_JSON_NAME = 'train_transforms.json'\n",
    "NOTEBOOK_NAME = 'temporal_anomaly_detection.ipynb'\n",
    "ARCHITECTURE_VISUALIZATION_NAME = 'architecture'\n",
    "ROC_CHART_NAME = 'roc_chart.png'\n",
    "\n",
    "# DATASET\n",
    "# ----------------------------------------\n",
    "DATASET_DIR = Path().home() / f'source/driver-dataset/{DATASET_NAME}'\n",
    "\n",
    "NORMAL_MEMORY_MAP = DATASET_DIR / 'normal' / 'memory_maps' / f'depth_{IMAGE_SIZE}.dat'\n",
    "ANOMAL_MEMORY_MAP = DATASET_DIR / 'anomal' / 'memory_maps' / f'depth_{IMAGE_SIZE}.dat'\n",
    "ANOMALIES_FILE = DATASET_DIR / 'anomal' / 'labels.txt'\n",
    "\n",
    "assert (\n",
    "    NORMAL_MEMORY_MAP.exists()\n",
    "), f'Normal memory map does not exist: {NORMAL_MEMORY_MAP}'\n",
    "assert (\n",
    "    ANOMAL_MEMORY_MAP.exists()\n",
    "), f'Anomal memory map does not exist: {ANOMAL_MEMORY_MAP}'\n",
    "assert ANOMALIES_FILE.exists(), f'Anomalies file does not exist: {ANOMALIES_FILE}'\n",
    "\n",
    "if MODEL_NAME == 'tae3d':\n",
    "    assert (\n",
    "        TIME_STEPS >= 16\n",
    "    ), 'Number of time steps must be at least 16 for the 3D convolution model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test case for forward pass, also used as model signature in Mlflow.\n",
    "\n",
    "if MODEL_NAME == 'tae':\n",
    "    encoder = LSTMEncoder(n_time_steps=TIME_STEPS, bidirectional=True)\n",
    "    decoder = LSTMDecoder(\n",
    "        n_time_steps=TIME_STEPS,\n",
    "        n_image_channels=1,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        bidirectional=True,\n",
    "    )\n",
    "\n",
    "    # Test input tensor of size (batch_size, time_steps, channels, height, width)\n",
    "    INPUT = torch.randn(BATCH_SIZE, TIME_STEPS, 1, IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "    # Forward pass through the encoder and decoder\n",
    "    encoded = encoder(INPUT)\n",
    "    decoded = decoder(encoded)\n",
    "\n",
    "    # Check the shapes\n",
    "    print(f'Input shape: {INPUT.shape}')\n",
    "    print(f'Latent shape: {encoded.shape}')\n",
    "    print(f'Decoded shape: {decoded.shape}')\n",
    "\n",
    "    assert INPUT.shape == decoded.shape, 'Input and output shapes do not match!'\n",
    "\n",
    "    print(summarize_model([encoder, decoder]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test case for forward pass, also used as model signature in Mlflow.\n",
    "\n",
    "if MODEL_NAME == 'tae3d':\n",
    "    encoder = Conv3dEncoder()\n",
    "    decoder = Conv3dDecoder()\n",
    "\n",
    "    # Test input tensor of size (batch_size, channels, time_steps/depth, height, width)\n",
    "    INPUT = torch.randn(BATCH_SIZE, 1, TIME_STEPS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "    # Forward pass through the encoder and decoder\n",
    "    encoded = encoder(INPUT)\n",
    "    decoded = decoder(encoded)\n",
    "\n",
    "    # Check the shapes\n",
    "    print(f'Input shape: {INPUT.shape}')\n",
    "    print(f'Latent shape: {encoded.shape}')\n",
    "    print(f'Decoded shape: {decoded.shape}')\n",
    "\n",
    "    assert INPUT.shape == decoded.shape, 'Input and output shapes do not match!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchview.draw_graph(\n",
    "#     Conv3dAutoencoder(),\n",
    "#     input_size=(32, 1, 16, IMAGE_SIZE, IMAGE_SIZE),\n",
    "#     graph_dir='TB',\n",
    "#     depth=3,\n",
    "#     roll=True,\n",
    "#     expand_nested=True,\n",
    "#     graph_name='Temporal Autoencoder',\n",
    "#     save_graph=True,\n",
    "#     filename='tae3d',\n",
    "# ).visual_graph.render(format='svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_dict = BatchSizeDict(\n",
    "    {'train': BATCH_SIZE, 'valid': BATCH_SIZE, 'test': BATCH_SIZE}\n",
    ")\n",
    "\n",
    "train_val_dataset = TemporalAutoencoderDataset(\n",
    "    memory_map_file=NORMAL_MEMORY_MAP,\n",
    "    memory_map_image_shape=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    window_size=TIME_STEPS,\n",
    "    time_dim_index=0 if MODEL_NAME == 'tae' else 1,\n",
    ")\n",
    "test_dataset = TemporalAutoencoderDataset(\n",
    "    memory_map_file=ANOMAL_MEMORY_MAP,\n",
    "    memory_map_image_shape=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    window_size=TIME_STEPS,\n",
    "    time_dim_index=0 if MODEL_NAME == 'tae' else 1,\n",
    ")\n",
    "\n",
    "# Train validation split\n",
    "train_size = int(TRAIN_SET_RATIO * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size_dict['train'],\n",
    "    shuffle=False,  # YOU DON'T WANT TO SHUFFLE TEMPORAL DATA!\n",
    "    num_workers=int(os.cpu_count()),  # type: ignore\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size_dict['valid'],\n",
    "    shuffle=False,\n",
    "    num_workers=int(os.cpu_count()),  # type: ignore\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,  # batch_size_dict['test'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # int(os.cpu_count()),  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalAutoencoderModel(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    batch_size_dict=batch_size_dict,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    loss_function=LOSS_FUNCTION,\n",
    "    time_dim_index=1 if MODEL_NAME == 'tae' else 2,\n",
    "    train_noise_std_input=TRAIN_NOISE_STD_INPUT,\n",
    "    train_noise_std_latent=TRAIN_NOISE_STD_LATENT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(LOG_DIR, name=EXPERIMENT_NAME, version=VERSION)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=MONITOR,\n",
    "    mode='min',\n",
    "    patience=PATIENCE,\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=EXPERIMENT_DIR,\n",
    "    filename='{epoch}-{val_loss:3f}',\n",
    "    monitor=MONITOR,\n",
    "    save_top_k=1,  # save only the best model\n",
    "    mode='min',\n",
    ")\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',\n",
    "    logger=csv_logger,\n",
    "    callbacks=[model_checkpoint, early_stopping, progress_bar],\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    log_every_n_steps=1,  # log every batch\n",
    "    # https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility\n",
    "    deterministic=True,\n",
    ")\n",
    "torch.use_deterministic_algorithms(\n",
    "    True, warn_only=True\n",
    ")  # torch 2.5 does not have a deterministic implementation of `max_pool3d_with_indices_backward_cuda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Load from MLflow\n",
    "    model_name = 'pytorch-2024-11-02-073717-tae'\n",
    "    model_version = 1\n",
    "    model_uri = f'models:/{model_name}/{model_version}'\n",
    "    model_ = mlflow.pytorch.load_model(model_uri)\n",
    "else:\n",
    "    model_checkpoint_path = list(EXPERIMENT_DIR.glob('*.ckpt'))[0]\n",
    "    model_ = TemporalAutoencoderModel.load_from_checkpoint(\n",
    "        model_checkpoint_path, encoder=encoder, decoder=decoder\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_ = L.Trainer(logger=False)  # no need to log anything for validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metrics = trainer_.validate(model_, dataloaders=valid_dataloader, verbose=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = trainer_.test(model_, dataloaders=test_dataloader, verbose=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = Anomalies.from_file(ANOMALIES_FILE)\n",
    "n_test_frames = len(list((ANOMALIES_FILE.parent / 'images').glob('*.jpg')))\n",
    "y_true = anomalies.to_ground_truth(n_test_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "errors = defaultdict(list)\n",
    "model_.eval()\n",
    "model_.to(device)\n",
    "for i in tqdm(range(len(test_dataloader))):\n",
    "    input_seq = test_dataloader.dataset[i]['image']\n",
    "    with torch.no_grad():\n",
    "        res = model_(input_seq.unsqueeze(0).to(device))[0]\n",
    "\n",
    "    res = res.cpu()\n",
    "    mse = (\n",
    "        F.mse_loss(res, input_seq, reduction='none').mean(dim=1).squeeze().mean().item()\n",
    "    )\n",
    "    mae = (\n",
    "        F.l1_loss(res, input_seq, reduction='none').mean(dim=1).squeeze().mean().item()\n",
    "    )\n",
    "    fro = torch.norm((res - input_seq).squeeze(), p='fro', dim=0).mean().item()\n",
    "\n",
    "    errors['mse'].append(mse)\n",
    "    errors['mae'].append(mae)\n",
    "    errors['fro'].append(fro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_proba_from_errors(errors: list[float]) -> list[float]:\n",
    "    \"\"\"Normalize errors to [0, 1] range and get probabilistic `y_pred`.\"\"\"\n",
    "    min_err = min(errors)\n",
    "    max_err = max(errors)\n",
    "    errors_norm = [(x - min_err) / (max_err - min_err) for x in errors]\n",
    "    y_proba = list(chain.from_iterable([[x] * TIME_STEPS for x in errors_norm]))\n",
    "    return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ROC AUC score for each metric, choose the best one.\n",
    "best_metric = 'mse'\n",
    "best_roc_auc_score = 0.0\n",
    "for key, value in errors.items():\n",
    "    y_proba = get_y_proba_from_errors(value)\n",
    "    roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    if roc_auc > best_roc_auc_score:\n",
    "        best_metric = key\n",
    "        best_roc_auc_score = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc, optimal_threshold = plot_roc_chart(\n",
    "    y_true,\n",
    "    get_y_proba_from_errors(errors[best_metric]),\n",
    "    save_path=EXPERIMENT_DIR / ROC_CHART_NAME,\n",
    "    cbar_text=f'Thresholds ({best_metric.upper()})',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_temporal_autoencoder_reconstruction(\n",
    "    model_,\n",
    "    test_dataloader,\n",
    "    save_path=EXPERIMENT_DIR / PREDICTIONS_PNG_NAME,\n",
    "    time_dim_index=0 if MODEL_NAME == 'tae' else 1,\n",
    "    indices=[0]\n",
    "    + [anomalies[0].middle() // TIME_STEPS]\n",
    "    + [anomalies[2].middle() // TIME_STEPS],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(\n",
    "    EXPERIMENT_DIR / METRICS_CSV_NAME,\n",
    "    save_path=EXPERIMENT_DIR / LEARNING_CURVES_PDF_NAME,\n",
    "    metrics={\n",
    "        'mse': 'Mean Squared Error',\n",
    "        'fro': 'Frobenius Norm',\n",
    "        'mae': 'Mean Absolute Error',\n",
    "    },\n",
    "    figsize=(22, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_visualization = torchview.draw_graph(\n",
    "    model_,\n",
    "    input_size=INPUT.shape,\n",
    "    graph_dir='TB',\n",
    "    depth=3,\n",
    "    roll=True,\n",
    "    expand_nested=True,\n",
    "    graph_name='Temporal Autoencoder',\n",
    "    save_graph=False,\n",
    "    filename=ARCHITECTURE_VISUALIZATION_NAME,\n",
    "    directory=str(EXPERIMENT_DIR),\n",
    ")\n",
    "architecture_visualization.visual_graph.render(format='svg')\n",
    "architecture_visualization.visual_graph.render(format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_early_stopping_epoch() -> int | None:\n",
    "    checkpoint = list(EXPERIMENT_DIR.glob('*.ckpt'))[0].stem\n",
    "    pattern = r'epoch=(\\d+)'\n",
    "    match = re.search(pattern, checkpoint)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dict_to_mlflow(\n",
    "    dictionary: Mapping[str, int | float], type: Literal['metric', 'param']\n",
    ") -> None:\n",
    "    for k, v in dictionary.items():\n",
    "        if type == 'metric':\n",
    "            mlflow.log_metric(k, v)\n",
    "        elif type == 'param':\n",
    "            mlflow.log_param(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submodule_param_count(model: L.LightningModule) -> dict[str, int]:\n",
    "    \"\"\"Get the number of parameters for each submodule in the model.\"\"\"\n",
    "    param_counts = {}\n",
    "    for name, submodule in model.named_children():\n",
    "        num_params = sum(p.numel() for p in submodule.parameters())\n",
    "        if num_params > 0:\n",
    "            param_counts[f'{name}_parameters'] = num_params\n",
    "    param_counts['total_parameters'] = sum([x for x in param_counts.values()])\n",
    "    return param_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=f'{EXPERIMENT_NAME}') as run:\n",
    "    try:\n",
    "        mlflow.set_tag('Branch', get_current_branch())\n",
    "        mlflow.set_tag('Commit ID', get_commit_id())\n",
    "        mlflow.set_tag('Dataset', DATASET_NAME)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    mlflow.log_metric('roc_auc', roc_auc)\n",
    "    mlflow.log_metric('optimal_threshold', optimal_threshold)\n",
    "    log_dict_to_mlflow(valid_metrics, 'metric')\n",
    "    log_dict_to_mlflow(test_metrics, 'metric')\n",
    "    log_dict_to_mlflow(get_submodule_param_count(model_), 'param')\n",
    "\n",
    "    mlflow.log_param('encoder', str(encoder))\n",
    "    mlflow.log_param('decoder', str(decoder))\n",
    "    mlflow.log_param('batch_size', BATCH_SIZE)\n",
    "    mlflow.log_param('max_epochs', MAX_EPOCHS)\n",
    "    mlflow.log_param('early_stopping', get_early_stopping_epoch())\n",
    "    mlflow.log_param('monitor', MONITOR)\n",
    "    mlflow.log_param('patience', PATIENCE)\n",
    "    mlflow.log_param('image_size', IMAGE_SIZE)\n",
    "    mlflow.log_param('learning_rate', LEARNING_RATE)\n",
    "    mlflow.log_param('loss_function', LOSS_FUNCTION)\n",
    "    mlflow.log_param('train_noise_std_input', TRAIN_NOISE_STD_INPUT)\n",
    "    mlflow.log_param('train_noise_std_latent', TRAIN_NOISE_STD_LATENT)\n",
    "    mlflow.log_param('seed', SEED)\n",
    "    mlflow.log_param('time_steps', TIME_STEPS)\n",
    "    mlflow.log_param('train_set_ratio', TRAIN_SET_RATIO)\n",
    "\n",
    "    # CSV metrics, learning curves, predictions, notebook\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / METRICS_CSV_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / LEARNING_CURVES_PDF_NAME), MLFLOW_ARTIFACT_DIR\n",
    "    )\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / PREDICTIONS_PNG_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(str(EXPERIMENT_DIR / ROC_CHART_NAME), MLFLOW_ARTIFACT_DIR)\n",
    "    mlflow.log_artifact(NOTEBOOK_NAME, MLFLOW_ARTIFACT_DIR)\n",
    "\n",
    "    # Network visualization with `torchview`\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / (ARCHITECTURE_VISUALIZATION_NAME + '.svg')),\n",
    "        MLFLOW_ARTIFACT_DIR,\n",
    "    )\n",
    "    mlflow.log_artifact(\n",
    "        str(EXPERIMENT_DIR / (ARCHITECTURE_VISUALIZATION_NAME + '.png')),\n",
    "        MLFLOW_ARTIFACT_DIR,\n",
    "    )\n",
    "\n",
    "    # Models are versioned by default\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model=model_,\n",
    "        artifact_path='model',\n",
    "        registered_model_name=f'pytorch-{EXPERIMENT_NAME}',\n",
    "        signature=infer_signature(INPUT.numpy(), INPUT.numpy(), dict(training=False)),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
